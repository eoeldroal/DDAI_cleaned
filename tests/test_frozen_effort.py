import os
import asyncio
import time
import base64
import json
import random
from openai import AsyncOpenAI
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

# ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì„¤ì •
MAX_CONCURRENT = 128  # ë™ì‹œ ìš”ì²­ ìˆ˜
TOTAL_REQUESTS = 128  # ì´ ìš”ì²­ ìˆ˜

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

async def run_single_test(client, model, effort, question, image_path):
    print(f"\n--- Testing with Effort: {effort} ---")
    
    # ì´ë¯¸ì§€ ì¸ì½”ë”©
    base64_image = encode_image(image_path)
    
    # ì…ë ¥ êµ¬ì„± (generation.py ìŠ¤íƒ€ì¼)
    sys_prompt = (
        "You are a visual QA generator. "
        "Use only the provided images and the user question. "
        "Return ONLY the final answer text without extra explanations."
    )
    
    user_content = [
        {
            "type": "input_image",
            "image_url": f"data:image/jpeg;base64,{base64_image}"
        },
        {
            "type": "input_text",
            "text": f"Question: {question}"
        }
    ]
    
    inputs = [
        {"role": "developer", "content": sys_prompt},
        {"role": "user", "content": user_content},
    ]

    start_time = time.perf_counter()
    try:
        # effortê°€ Noneì´ë©´ íŒŒë¼ë¯¸í„°ë¥¼ ì•„ì˜ˆ ëºŒ (ê¸°ë³¸ê°’ í…ŒìŠ¤íŠ¸)
        kwargs = {"model": model, "input": inputs, "max_output_tokens": 1024}
        if effort:
            kwargs["reasoning"] = {"effort": effort}

        response = await client.responses.create(**kwargs)
        
        duration = time.perf_counter() - start_time
        
        # ì‘ë‹µ ì¶”ì¶œ
        answer = getattr(response, "output_text", None)
        if not answer and getattr(response, "output", None):
            answer = str(response.output) # ê°„ëµí™”
            
        usage = getattr(response, "usage", None)
        
        print(f"Status: Success")
        print(f"Time: {duration:.4f}s")
        if usage:
            print(f"Tokens: Total={usage.total_tokens}, Input={usage.input_tokens}, Output={usage.output_tokens}")
            # reasoning_tokensê°€ ìˆëŠ”ì§€ í™•ì¸ (OpenAI í‘œì¤€)
            if hasattr(usage, 'output_tokens_details'):
                details = usage.output_tokens_details
                if hasattr(details, 'reasoning_tokens'):
                    print(f"  -> Reasoning Tokens: {details.reasoning_tokens}")
        print(f"Answer Preview: {answer[:100]}...")
        
    except Exception as e:
        print(f"Status: Failed ({e})")
        return None, None, str(e)


async def run_single_request(
    client: AsyncOpenAI,
    model: str,
    effort: str,
    question: str,
    image_path: str,
    semaphore: asyncio.Semaphore,
    request_id: int,
) -> dict:
    """
    ì‹¤ì œ generation.py íŒ¨í„´ì„ ë”°ë¥´ëŠ” ë‹¨ì¼ ìš”ì²­ (ì„¸ë§ˆí¬ì–´ í¬í•¨)
    """
    async with semaphore:
        base64_image = encode_image(image_path)

        sys_prompt = (
            "You are a visual QA generator. "
            "Use only the provided images and the user question. "
            "Return ONLY the final answer text without extra explanations."
        )

        user_content = [
            {
                "type": "input_image",
                "image_url": f"data:image/jpeg;base64,{base64_image}"
            },
            {
                "type": "input_text",
                "text": f"Question: {question}"
            }
        ]

        inputs = [
            {"role": "developer", "content": sys_prompt},
            {"role": "user", "content": user_content},
        ]

        start_time = time.perf_counter()
        try:
            kwargs = {"model": model, "input": inputs, "max_output_tokens": 1024}
            if effort:
                kwargs["reasoning"] = {"effort": effort}

            response = await client.responses.create(**kwargs)
            duration = time.perf_counter() - start_time

            answer = getattr(response, "output_text", None)
            usage = getattr(response, "usage", None)

            reasoning_tokens = 0
            if usage and hasattr(usage, 'output_tokens_details'):
                details = usage.output_tokens_details
                if hasattr(details, 'reasoning_tokens'):
                    reasoning_tokens = details.reasoning_tokens

            return {
                "request_id": request_id,
                "status": "success",
                "duration": duration,
                "total_tokens": usage.total_tokens if usage else 0,
                "input_tokens": usage.input_tokens if usage else 0,
                "output_tokens": usage.output_tokens if usage else 0,
                "reasoning_tokens": reasoning_tokens,
                "answer_preview": (answer[:50] + "...") if answer and len(answer) > 50 else answer,
            }
        except Exception as e:
            duration = time.perf_counter() - start_time
            return {
                "request_id": request_id,
                "status": "failed",
                "duration": duration,
                "error": str(e),
            }


async def run_parallel_test(client: AsyncOpenAI, model: str, effort: str, test_samples: list):
    """
    generation.pyì˜ asyncio.gather() íŒ¨í„´ì„ ë”°ë¥´ëŠ” ë³‘ë ¬ í…ŒìŠ¤íŠ¸
    """
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)

    print(f"\n{'='*60}")
    print(f"ğŸš€ ë³‘ë ¬ ìš”ì²­ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"   ì´ ìš”ì²­ ìˆ˜: {len(test_samples)}")
    print(f"   ë™ì‹œ ìš”ì²­ ìˆ˜ (ì„¸ë§ˆí¬ì–´): {MAX_CONCURRENT}")
    print(f"   Reasoning Effort: {effort}")
    print(f"{'='*60}")

    # íƒœìŠ¤í¬ ìƒì„± (generation.py íŒ¨í„´)
    tasks = [
        run_single_request(
            client=client,
            model=model,
            effort=effort,
            question=sample["question"],
            image_path=sample["image_path"],
            semaphore=semaphore,
            request_id=i,
        )
        for i, sample in enumerate(test_samples)
    ]

    # ì „ì²´ ì‹œê°„ ì¸¡ì •
    total_start = time.perf_counter()

    # asyncio.gatherë¡œ ë³‘ë ¬ ì²˜ë¦¬ (generation.py íŒ¨í„´)
    results = await asyncio.gather(*tasks, return_exceptions=True)

    total_duration = time.perf_counter() - total_start

    # ê²°ê³¼ ë¶„ì„
    success_count = 0
    failed_count = 0
    durations = []
    total_tokens_sum = 0
    reasoning_tokens_sum = 0

    for r in results:
        if isinstance(r, Exception):
            failed_count += 1
        elif isinstance(r, dict):
            if r.get("status") == "success":
                success_count += 1
                durations.append(r["duration"])
                total_tokens_sum += r.get("total_tokens", 0)
                reasoning_tokens_sum += r.get("reasoning_tokens", 0)
            else:
                failed_count += 1

    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"   ì„±ê³µ: {success_count}/{len(test_samples)}")
    print(f"   ì‹¤íŒ¨: {failed_count}/{len(test_samples)}")
    print(f"\nâ±ï¸  ì‹œê°„ í†µê³„:")
    print(f"   ì „ì²´ ì†Œìš” ì‹œê°„: {total_duration:.2f}ì´ˆ")
    if durations:
        avg_duration = sum(durations) / len(durations)
        min_duration = min(durations)
        max_duration = max(durations)
        print(f"   ê°œë³„ ìš”ì²­ í‰ê· : {avg_duration:.2f}ì´ˆ")
        print(f"   ê°œë³„ ìš”ì²­ ìµœì†Œ: {min_duration:.2f}ì´ˆ")
        print(f"   ê°œë³„ ìš”ì²­ ìµœëŒ€: {max_duration:.2f}ì´ˆ")
        print(f"   ì²˜ë¦¬ëŸ‰: {len(test_samples) / total_duration:.2f} ìš”ì²­/ì´ˆ")

    print(f"\nğŸ”¢ í† í° í†µê³„:")
    print(f"   ì´ í† í°: {total_tokens_sum:,}")
    print(f"   Reasoning í† í°: {reasoning_tokens_sum:,}")
    if success_count > 0:
        print(f"   í‰ê·  í† í°/ìš”ì²­: {total_tokens_sum / success_count:.0f}")
        print(f"   í‰ê·  Reasoning í† í°/ìš”ì²­: {reasoning_tokens_sum / success_count:.0f}")

    return {
        "total_duration": total_duration,
        "success_count": success_count,
        "failed_count": failed_count,
        "avg_duration": sum(durations) / len(durations) if durations else 0,
        "throughput": len(test_samples) / total_duration if total_duration > 0 else 0,
        "total_tokens": total_tokens_sum,
        "reasoning_tokens": reasoning_tokens_sum,
    }


# ë¶„ì„ì—ì„œ ë°œê²¬í•œ ë¬¸ì œ ì¼€ì´ìŠ¤ë“¤
PROBLEM_CASES = [
    {
        "name": "Case 1: Hallucination - ì´ë¯¸ì§€ì— ì—†ëŠ” ì •ë³´",
        "question": "What are the types of processing used by Uber?",
        "image_path": "./search_engine/corpus/img/8273_8.jpg",  # UBER ê´€ë ¨ ì´ë¯¸ì§€
        "expected_issue": "hallucination",
        "note": "ì´ë¯¸ì§€ì— ì²˜ë¦¬ ìœ í˜• ì •ë³´ê°€ ì—†ìœ¼ë©´ 'I don't know'ë¼ê³  í•´ì•¼ í•¨"
    },
    {
        "name": "Case 2: ìˆ˜ì¹˜ ì¶”ì¶œ - í‘œì—ì„œ ê°’ ì¶”ì¶œ",
        "question": 'According to the table on talent adaptability score, what is the difference in the "Average number of employees" between France and Australia?',
        "image_path": "./search_engine/corpus/img/4426_7.jpg",
        "expected_issue": "numeric_extraction",
        "note": "ì •í™•í•œ ìˆ˜ì¹˜ ê³„ì‚° í•„ìš” (0.2ê°€ ì •ë‹µ)"
    },
    {
        "name": "Case 3: ë³µì¡í•œ ì¡°ê±´ë¶€ ì§ˆë¬¸",
        "question": "What percentage of those surveyed did not report being a Housewife or a Student?",
        "image_path": "./search_engine/corpus/img/1084_7.jpg",  # ë² íŠ¸ë‚¨ ì˜¤í† ë°”ì´ ì†Œìœ ìœ¨ ì„¤ë¬¸
        "expected_issue": "wrong_context",
        "note": "ì´ë¯¸ì§€ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ì„ ë•Œ ì–´ë–»ê²Œ ë‹µí•˜ëŠ”ê°€"
    },
    {
        "name": "Case 4: ì°¨íŠ¸ í•´ì„",
        "question": "Which Altcoin has the highest value market cap?",
        "image_path": "./search_engine/corpus/img/8484_7.jpg",  # Cryptocurrency ê´€ë ¨
        "expected_issue": "concept_understanding",
        "note": "Bitcoinì€ Altcoinì´ ì•„ë‹˜ - ê°œë… ì´í•´ í•„ìš”"
    },
    {
        "name": "Case 5: ìƒ‰ìƒ ì¸ì‹",
        "question": "What is the background color of the two credit cards that are visible?",
        "image_path": "./search_engine/corpus/img/6263_2.jpg",  # ì‹ ìš©ì¹´ë“œ ì´ë¯¸ì§€
        "expected_issue": "visual_recognition",
        "note": "ì‹œê°ì  ìš”ì†Œ ì •í™•íˆ ì¸ì‹í•˜ëŠ”ê°€"
    },
]


def load_samples_from_log(jsonl_path: str, num_samples: int = 128) -> list:
    """
    frozen_generator_detail.jsonlì—ì„œ ì‹¤ì œ ìƒ˜í”Œ ë¡œë“œ
    """
    samples = []
    with open(jsonl_path, "r") as f:
        for line in f:
            if len(samples) >= num_samples:
                break
            try:
                record = json.loads(line.strip())
                question = record.get("question", "").replace("\nassistant\n", "").strip()
                image_paths = record.get("image_paths", [])

                # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œë§Œ ì‚¬ìš©
                valid_paths = [p for p in image_paths if os.path.exists(p)]
                if valid_paths and question:
                    samples.append({
                        "question": question,
                        "image_path": valid_paths[0],  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì‚¬ìš©
                    })
            except json.JSONDecodeError:
                continue
    return samples


async def main():
    import sys

    # ì„¤ì • ë¡œë“œ
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    model = "gpt-5-mini-2025-08-07"

    client = AsyncOpenAI(api_key=api_key, base_url=base_url)

    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "--parallel":
        # 128ê°œ ë³‘ë ¬ í…ŒìŠ¤íŠ¸
        print("=" * 60)
        print("GPT-5mini 128ê°œ ë³‘ë ¬ ìš”ì²­ í…ŒìŠ¤íŠ¸")
        print("Reasoning Effort: medium")
        print("=" * 60)

        # frozen_generator_detail.jsonlì—ì„œ ìƒ˜í”Œ ë¡œë“œ
        jsonl_path = "./logs/frozen_generator_detail.jsonl"
        if not os.path.exists(jsonl_path):
            print(f"Error: {jsonl_path} not found")
            return

        samples = load_samples_from_log(jsonl_path, TOTAL_REQUESTS)
        print(f"ğŸ“‚ ë¡œë“œëœ ìƒ˜í”Œ ìˆ˜: {len(samples)}")

        if len(samples) < TOTAL_REQUESTS:
            print(f"âš ï¸ ìš”ì²­ëœ {TOTAL_REQUESTS}ê°œë³´ë‹¤ ì ì€ {len(samples)}ê°œë§Œ ë¡œë“œë¨")

        # ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = await run_parallel_test(client, model, "medium", samples)

        # GRPO í›ˆë ¨ ì‹œê°„ ì˜ˆì¸¡
        print(f"\n{'='*60}")
        print("ğŸ“ˆ GRPO í›ˆë ¨ ì‹œê°„ ì˜ˆì¸¡ (Reasoning Effort = medium ê¸°ì¤€)")
        print(f"{'='*60}")

        # ê°€ì •: GRPOëŠ” ê° ìƒ˜í”Œë‹¹ 4ê°œ ì‘ë‹µ ìƒì„± (n_agent=4)
        n_agent = 4
        total_samples = 6277  # frozen_generator_detail.jsonl ì „ì²´

        # ì˜ˆì¸¡
        throughput = result["throughput"]
        avg_duration = result["avg_duration"]

        single_pass_requests = total_samples * n_agent
        estimated_time_sec = single_pass_requests / throughput if throughput > 0 else 0
        estimated_time_min = estimated_time_sec / 60

        print(f"   í…ŒìŠ¤íŠ¸ ì²˜ë¦¬ëŸ‰: {throughput:.2f} ìš”ì²­/ì´ˆ")
        print(f"   GRPO ë‹¨ì¼ íŒ¨ìŠ¤ ìš”ì²­ ìˆ˜: {single_pass_requests:,} ({total_samples} x {n_agent})")
        print(f"   ì˜ˆìƒ ë‹¨ì¼ íŒ¨ìŠ¤ ì†Œìš” ì‹œê°„: {estimated_time_min:.1f}ë¶„")
        print(f"   (ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„ {avg_duration * single_pass_requests / 60:.0f}ë¶„ -> {estimated_time_min:.1f}ë¶„)")

    else:
        # ê°œë³„ ë¬¸ì œ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        print("=" * 60)
        print("GPT-5mini Reasoning Effort í…ŒìŠ¤íŠ¸ (Medium)")
        print("ë¶„ì„ì—ì„œ ë°œê²¬ëœ ë¬¸ì œ ì¼€ì´ìŠ¤ ê²€ì¦")
        print("=" * 60)
        print("\nğŸ’¡ 128ê°œ ë³‘ë ¬ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´: python tests/test_frozen_effort.py --parallel")

        for case in PROBLEM_CASES:
            print(f"\n{'='*60}")
            print(f"ğŸ“‹ {case['name']}")
            print(f"   ì§ˆë¬¸: {case['question'][:80]}...")
            print(f"   ì´ë¯¸ì§€: {case['image_path']}")
            print(f"   ì˜ˆìƒ ì´ìŠˆ: {case['expected_issue']}")
            print(f"   ë…¸íŠ¸: {case['note']}")
            print("-" * 60)

            if not os.path.exists(case['image_path']):
                print(f"   âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ, ê±´ë„ˆëœ€")
                continue

            await run_single_test(
                client,
                model,
                "medium",  # reasoning effort = medium
                case['question'],
                case['image_path']
            )

            # API rate limit ë°©ì§€
            await asyncio.sleep(1)

        print("\n" + "=" * 60)
        print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
