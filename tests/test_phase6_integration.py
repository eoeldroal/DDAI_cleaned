#!/usr/bin/env python3
"""
Phase 6 í†µí•© í…ŒìŠ¤íŠ¸

ì‹¤ì œ APIë¥¼ ì‚¬ìš©í•œ End-to-End í…ŒìŠ¤íŠ¸:
1. Frozen Generator ë°°ì¹˜ í˜¸ì¶œ í…ŒìŠ¤íŠ¸
2. ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
3. ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
"""

import os
import sys
import asyncio
import threading
import time
from typing import Dict, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# .env ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env'))


def test_frozen_generator_batch_call():
    """Frozen Generator ë°°ì¹˜ í˜¸ì¶œ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("1. Frozen Generator ë°°ì¹˜ í˜¸ì¶œ í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        from vrag_agent.generation import LLMGenerationManager, GenerationConfig

        # Mock processor ìƒì„±
        class MockProcessor:
            class Tokenizer:
                pad_token_id = 0
            tokenizer = Tokenizer()

        config = GenerationConfig(
            max_turns=5,
            max_prompt_length=4096,
            num_gpus=8,
            search_url="http://localhost:5002/search",
            frozen_model="qwen2.5-vl-72b-instruct",
            frozen_max_tokens=256,
            frozen_max_concurrent=10,
        )

        manager = LLMGenerationManager(
            processor=MockProcessor(),
            actor_rollout_wg=None,
            config=config,
            is_validation=False,
            streaming_reward_manager=None
        )

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        indices = [0, 1, 2]
        questions = [
            "What is 5 + 5?",
            "What is the capital of France?",
            "What is 2 * 3?",
        ]
        images_list = [[], [], []]  # ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ í…ŒìŠ¤íŠ¸

        print("  í˜¸ì¶œ ì¤‘...")
        start_time = time.perf_counter()

        # ë°°ì¹˜ í˜¸ì¶œ
        results = manager._call_frozen_generator_batch(indices, questions, images_list)

        elapsed = time.perf_counter() - start_time

        print(f"  ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print("  ê²°ê³¼:")

        success_count = 0
        for idx, q in zip(indices, questions):
            answer = results.get(idx, "")
            status = "âœ…" if answer else "âŒ"
            if answer:
                success_count += 1
            print(f"    {status} [{idx}] {q}")
            print(f"        â†’ {answer[:100]}..." if len(answer) > 100 else f"        â†’ {answer}")

        print(f"\n  ì„±ê³µë¥ : {success_count}/{len(indices)}")

        if success_count == len(indices):
            print("âœ… Frozen Generator ë°°ì¹˜ í˜¸ì¶œ í…ŒìŠ¤íŠ¸ í†µê³¼")
            print()
            return True
        else:
            print("âŒ ì¼ë¶€ ì‹¤íŒ¨")
            return False

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_background_pipeline_simulation():
    """ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("2. ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        from vrag_agent.generation import LLMGenerationManager, GenerationConfig

        # Mock processor ìƒì„±
        class MockProcessor:
            class Tokenizer:
                pad_token_id = 0
            tokenizer = Tokenizer()

        config = GenerationConfig(
            max_turns=5,
            max_prompt_length=4096,
            num_gpus=8,
            search_url="http://localhost:5002/search",
            frozen_model="qwen2.5-vl-72b-instruct",
            frozen_max_tokens=128,
            frozen_max_concurrent=20,
        )

        manager = LLMGenerationManager(
            processor=MockProcessor(),
            actor_rollout_wg=None,
            config=config,
            is_validation=False,
            streaming_reward_manager=None
        )

        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì„¤ì •
        manager.questions = ["What is 100 + 200?", "What is 300 + 400?"]
        manager.retrievaled_images = [[], []]
        manager.cropped_images = [[], []]
        manager.generated_answers = {}
        manager._streaming_frozen_generated = set()
        manager._pending_threads = []

        # Mock streaming reward manager
        class MockStreamingRewardManager:
            def __init__(self):
                self.submissions = []

            def submit_prompt(self, uid, sample_indices, samples_data):
                self.submissions.append({
                    'uid': uid,
                    'sample_indices': sample_indices,
                    'samples_data': samples_data
                })
                print(f"  [Mock] submit_prompt called: uid={uid}, indices={sample_indices}")

        mock_rm = MockStreamingRewardManager()
        manager.streaming_reward_manager = mock_rm

        # ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        indices = [0, 1]
        prompt_id = "test_prompt_001"
        status = {'completed_samples': 2, 'total_samples': 2, 'submitted': False}

        print("  ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘...")
        start_time = time.perf_counter()

        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì²˜ë¦¬
        thread = threading.Thread(
            target=manager._process_prompt_background,
            args=(indices, prompt_id, status),
            daemon=True
        )
        thread.start()

        spawn_time = time.perf_counter() - start_time
        print(f"  ìŠ¤ë ˆë“œ ì‹œì‘ ì‹œê°„: {spawn_time:.4f}ì´ˆ (ë¸”ë¡œí‚¹ ì—†ìŒ)")

        # ë©”ì¸ ìŠ¤ë ˆë“œ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
        print("  ë©”ì¸ ìŠ¤ë ˆë“œ: ë‹¤ë¥¸ ì‘ì—… ìˆ˜í–‰ ì¤‘... (ì‹œë®¬ë ˆì´ì…˜)")
        time.sleep(0.1)
        print("  ë©”ì¸ ìŠ¤ë ˆë“œ: ì‘ì—… ì™„ë£Œ")

        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        thread.join(timeout=60)

        total_time = time.perf_counter() - start_time

        # ê²°ê³¼ ê²€ì¦
        print("\n  ê²°ê³¼ ê²€ì¦:")
        print(f"    generated_answers: {manager.generated_answers}")
        print(f"    _streaming_frozen_generated: {manager._streaming_frozen_generated}")
        print(f"    submit_prompt í˜¸ì¶œ íšŸìˆ˜: {len(mock_rm.submissions)}")

        if mock_rm.submissions:
            submission = mock_rm.submissions[0]
            print(f"    ì œì¶œëœ samples_data:")
            for i, data in enumerate(submission['samples_data']):
                print(f"      [{i}] query: {data['query']}")
                print(f"          generated_answer: {data.get('generated_answer', 'N/A')[:50]}...")

        # ê²€ì¦ ì¡°ê±´
        success = (
            len(manager.generated_answers) == 2 and
            len(manager._streaming_frozen_generated) == 2 and
            len(mock_rm.submissions) == 1 and
            'generated_answer' in mock_rm.submissions[0]['samples_data'][0]
        )

        print(f"\n  ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")

        if success:
            print("âœ… ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ í†µê³¼")
            print()
            return True
        else:
            print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return False

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_multiple_prompts_concurrent():
    """ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("3. ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        from vrag_agent.generation import LLMGenerationManager, GenerationConfig

        # Mock processor ìƒì„±
        class MockProcessor:
            class Tokenizer:
                pad_token_id = 0
            tokenizer = Tokenizer()

        config = GenerationConfig(
            max_turns=5,
            max_prompt_length=4096,
            num_gpus=8,
            search_url="http://localhost:5002/search",
            frozen_model="qwen2.5-vl-72b-instruct",
            frozen_max_tokens=64,
            frozen_max_concurrent=30,
        )

        manager = LLMGenerationManager(
            processor=MockProcessor(),
            actor_rollout_wg=None,
            config=config,
            is_validation=False,
            streaming_reward_manager=None
        )

        # 3ê°œ í”„ë¡¬í”„íŠ¸ ì‹œë®¬ë ˆì´ì…˜ (ê° 2ê°œ ìƒ˜í”Œ)
        num_prompts = 3
        samples_per_prompt = 2

        manager.questions = [
            "What is 1+1?", "What is 2+2?",  # Prompt 0
            "What is 3+3?", "What is 4+4?",  # Prompt 1
            "What is 5+5?", "What is 6+6?",  # Prompt 2
        ]
        manager.retrievaled_images = [[] for _ in range(6)]
        manager.cropped_images = [[] for _ in range(6)]
        manager.generated_answers = {}
        manager._streaming_frozen_generated = set()
        manager._pending_threads = []

        # Mock streaming reward manager
        class MockStreamingRewardManager:
            def __init__(self):
                self.submissions = []
                self.lock = threading.Lock()

            def submit_prompt(self, uid, sample_indices, samples_data):
                with self.lock:
                    self.submissions.append({
                        'uid': uid,
                        'sample_indices': sample_indices,
                        'samples_data': samples_data
                    })
                print(f"  [Mock] ì œì¶œ: {uid}")

        mock_rm = MockStreamingRewardManager()
        manager.streaming_reward_manager = mock_rm

        # ë™ì‹œì— 3ê°œ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
        print("  3ê°œ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬ ì‹œì‘...")
        start_time = time.perf_counter()

        for p_idx in range(num_prompts):
            base_idx = p_idx * samples_per_prompt
            indices = list(range(base_idx, base_idx + samples_per_prompt))
            prompt_id = f"prompt_{p_idx}"
            status = {'completed_samples': 2, 'total_samples': 2, 'submitted': False}

            thread = threading.Thread(
                target=manager._process_prompt_background,
                args=(indices, prompt_id, status),
                daemon=True,
                name=f"FrozenGen-{prompt_id}"
            )
            thread.start()
            manager._pending_threads.append(thread)

        spawn_time = time.perf_counter() - start_time
        print(f"  ìŠ¤ë ˆë“œ ì‹œì‘ ì‹œê°„: {spawn_time:.4f}ì´ˆ")

        # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        print("  ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
        for t in manager._pending_threads:
            t.join(timeout=60)

        total_time = time.perf_counter() - start_time

        # ê²°ê³¼ ê²€ì¦
        print("\n  ê²°ê³¼:")
        print(f"    ì´ ìƒì„±ëœ ë‹µë³€: {len(manager.generated_answers)}")
        print(f"    ì²˜ë¦¬ ì™„ë£Œëœ ìƒ˜í”Œ: {len(manager._streaming_frozen_generated)}")
        print(f"    ì œì¶œëœ í”„ë¡¬í”„íŠ¸: {len(mock_rm.submissions)}")

        for idx in sorted(manager.generated_answers.keys()):
            answer = manager.generated_answers[idx][:30] + "..." if len(manager.generated_answers[idx]) > 30 else manager.generated_answers[idx]
            print(f"    [{idx}] {manager.questions[idx]} â†’ {answer}")

        print(f"\n  ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"  ë³‘ë ¬ ì²˜ë¦¬ íš¨ê³¼: {samples_per_prompt * num_prompts}ê°œ ìƒ˜í”Œì„ {total_time:.2f}ì´ˆì— ì²˜ë¦¬")

        # ê²€ì¦
        success = (
            len(manager.generated_answers) == 6 and
            len(mock_rm.submissions) == 3
        )

        if success:
            print("âœ… ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")
            print()
            return True
        else:
            print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return False

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("\n" + "=" * 60)
    print("  Phase 6 í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60 + "\n")

    results = {}

    # 1. Frozen Generator ë°°ì¹˜ í˜¸ì¶œ
    results['frozen_batch'] = test_frozen_generator_batch_call()

    # 2. ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜
    results['background_pipeline'] = test_background_pipeline_simulation()

    # 3. ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬
    results['concurrent_prompts'] = test_multiple_prompts_concurrent()

    # ê²°ê³¼ ìš”ì•½
    print("=" * 60)
    print("  í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    all_passed = True
    for name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {name}: {status}")
        if not passed:
            all_passed = False

    print()
    if all_passed:
        print("ğŸ‰ ëª¨ë“  Phase 6 í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("\nì™„ì „ ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜ê°€ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print("  - ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ Frozen Generator í˜¸ì¶œ âœ“")
        print("  - Thread-safeí•œ ê²°ê³¼ ì €ì¥ âœ“")
        print("  - generated_answerê°€ samples_dataì— í¬í•¨ âœ“")
        print("  - ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ë³‘ë ¬ ì²˜ë¦¬ âœ“")
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
