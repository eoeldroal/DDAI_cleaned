import torch
import re
import numpy as np
from collections import defaultdict, deque
import os
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from .tensor_helper import TensorHelper, TensorConfig
from verl import DataProto
from verl.utils.tracking import Tracking
import shutil
import requests
from transformers.image_processing_base import BatchFeature
from PIL import Image
from tqdm import tqdm
import json
#generator ìˆ˜ì •
import uuid

from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time as _time
import random as _random
import asyncio
import base64 
import aiohttp 

# â–¼â–¼â–¼[ì„±ëŠ¥ ì¸¡ì • ì¶”ê°€]â–¼â–¼â–¼ ìˆ˜ì •
# GPUMonitorì™€ ì‹œê°„ ê¸°ë¡ì„ ìœ„í•œ ëª¨ë“ˆì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from lsm_tmp.gpu_monitor import GPUMonitor
from datetime import datetime
# â–²â–²â–²[ì„±ëŠ¥ ì¸¡ì • ì¶”ê°€]â–²â–²â–²

# =============================================================================
# [Phase 2] í…ì„œ ì—°ì‚° ìµœì í™”ìš© ë¡œê¹… ìœ í‹¸ë¦¬í‹°
# =============================================================================
import logging

# Phase 2 ì „ìš© ë¡œê±° ì„¤ì •
_phase2_logger = logging.getLogger("generation.phase2_optimization")
_phase2_logger.setLevel(logging.DEBUG)

# ì½˜ì†” í•¸ë“¤ëŸ¬ (INFO ë ˆë²¨ ì´ìƒ)
if not _phase2_logger.handlers:
    _console_handler = logging.StreamHandler()
    _console_handler.setLevel(logging.INFO)
    _console_handler.setFormatter(logging.Formatter(
        '[Phase2] %(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    ))
    _phase2_logger.addHandler(_console_handler)

# ì„±ëŠ¥ ì¸¡ì • í™˜ê²½ë³€ìˆ˜ í”Œë˜ê·¸ (ê¸°ë³¸ê°’: False)
_PHASE2_PERF_LOG_ENABLED = os.getenv("PHASE2_PERF_LOG", "0") == "1"

# =============================================================================
# [NEW] Frozen Generator ìƒì„¸ ë¡œê¹… ì„¤ì •
# =============================================================================
_FROZEN_GEN_LOG_PATH = os.path.join("./logs", "frozen_generator_detail.jsonl")
os.makedirs(os.path.dirname(_FROZEN_GEN_LOG_PATH), exist_ok=True)

def _log_frozen_generator_detail(
    idx: int,
    question: str,
    image_paths: List[str],
    answer: str,
    status_code: int,
    error: str = None
):
    """
    [NEW] Frozen Generator ìƒì„¸ ë¡œê¹…

    ì…ë ¥ í”„ë¡¬í”„íŠ¸ì™€ ì¶œë ¥ ì‘ë‹µì„ JSONL íŒŒì¼ê³¼ ì½˜ì†”ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    import datetime

    log_entry = {
        'timestamp': datetime.datetime.now().isoformat(),
        'sample_idx': idx,
        'question': question,
        'image_paths': image_paths,
        'answer': answer,
        'status_code': status_code,
        'error': error
    }

    # ì½˜ì†” ì¶œë ¥ (ê°„ëµ ë²„ì „)
    status = "SUCCESS" if status_code == 200 and not error else f"ERROR: {error or f'code={status_code}'}"
    print(f"\n{'='*60}")
    print(f"[Frozen Generator] Sample {idx} | Status: {status}")
    print(f"  Question: {question[:100]}{'...' if len(question) > 100 else ''}")
    print(f"  Images: {image_paths[:3]}{'...' if len(image_paths) > 3 else ''}")
    print(f"  Answer: {answer[:200]}{'...' if len(answer) > 200 else ''}")
    print(f"{'='*60}\n")

    # JSONL íŒŒì¼ì— ì €ì¥
    try:
        with open(_FROZEN_GEN_LOG_PATH, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    except Exception as e:
        print(f"[Frozen Generator] ìƒì„¸ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")


class Phase2PerfTimer:
    """Phase 2 ìµœì í™” í•¨ìˆ˜ì˜ ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""

    def __init__(self, func_name: str, batch_size: int = None):
        self.func_name = func_name
        self.batch_size = batch_size
        self.start_time = None
        self.elapsed_ms = None

    def __enter__(self):
        if _PHASE2_PERF_LOG_ENABLED:
            self.start_time = _time.perf_counter()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if _PHASE2_PERF_LOG_ENABLED and self.start_time:
            self.elapsed_ms = (_time.perf_counter() - self.start_time) * 1000
            batch_info = f" (batch_size={self.batch_size})" if self.batch_size else ""
            _phase2_logger.info(f"{self.func_name}{batch_info}: {self.elapsed_ms:.3f}ms")
        return False


# ===== (1) DashScope ì„¤ì • =====
from http import HTTPStatus
from dotenv import load_dotenv

# dotenv_dir = '/home/isdslab/sangmin/VRAG_test/'  # ê¸°ì¡´ í•˜ë“œì½”ë”© ê²½ë¡œ
dotenv_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # í”„ë¡œì íŠ¸ ë£¨íŠ¸

# 2. .env íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ ë§Œë“­ë‹ˆë‹¤.
dotenv_path = os.path.join(dotenv_dir, '.env')

# 3. í•´ë‹¹ ê²½ë¡œì˜ .env íŒŒì¼ì„ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv(dotenv_path=dotenv_path)

try:
    import dashscope  # frozen generator (Qwen2.5-VL-72B ê³„ì—´)
    import os as _os
    dashscope.base_http_api_url = _os.getenv(
        "DASHSCOPE_BASE_URL",
        "https://dashscope-intl.aliyuncs.com/api/v1"
    )
    _API_KEY = _os.getenv("DASHSCOPE_API_KEY") or _os.getenv("DASH_SCOPE_KEY")
    if not _API_KEY:
        raise RuntimeError("Set DASHSCOPE_API_KEY (or DASH_SCOPE_KEY).")
    dashscope.api_key = _API_KEY
    _HAS_DASHSCOPE = True
except Exception:
    _HAS_DASHSCOPE = False

# >>> ADDED: DashScope ë©€í‹°ëª¨ë‹¬ í—¬í¼ (import ë¸”ë¡ ë°”ë¡œ ì•„ë˜ì— ì¶”ê°€)
try:
    from dashscope import MultiModalConversation
except Exception:
    pass  # _HAS_DASHSCOPE=False ì¸ ê²½ìš° ëŒ€ë¹„

def _extract_text_from_multimodal(resp):
    """DashScope ë©€í‹°ëª¨ë‹¬ ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
    try:
        ot = getattr(resp, "output_text", None)
        if ot:
            return str(ot).strip()
    except Exception:
        pass

    out = getattr(resp, "output", None)
    if not isinstance(out, dict):
        return None

    choices = out.get("choices") or []
    if not choices:
        return None
    msg = choices[0].get("message") or {}
    content = msg.get("content") or []
    texts = []
    for part in content:
        if isinstance(part, dict) and part.get("text") is not None:
            texts.append(str(part["text"]))
    if texts:
        return "".join(texts).strip()

    if msg.get("text") is not None:
        return str(msg["text"]).strip()
    if out.get("text") is not None:
        return str(out["text"]).strip()
    return None


def _dashscope_call_with_fallback(model: str, messages: list, max_tokens: int):
    """SDK ë²„ì „ í˜¸í™˜: max_output_tokens â†’ ì‹¤íŒ¨ ì‹œ max_tokensë¡œ ì¬ì‹œë„"""
    try:
        return MultiModalConversation.call(
            model=model,
            messages=messages,
            max_output_tokens=max_tokens,
        )
    except TypeError:
        pass  # ì¼ë¶€ SDKëŠ” max_output_tokens ë¯¸ì§€ì›
    return MultiModalConversation.call(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
    )

def _to_image_part(path: str) -> dict | None:
    """ë¡œì»¬ ê²½ë¡œë¥¼ DashScope ì´ë¯¸ì§€ íŒŒíŠ¸(dict)ë¡œ ë³€í™˜ (file:// ìŠ¤í‚´ ê°•ì œ)"""
    if not path:
        return None
    if not path.startswith("file://"):
        path = "file://" + os.path.abspath(path)
    return {"image": path}
# <<< ADDED ë


# =============================================================================
# [Phase 5] OpenAI SDK ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ (Frozen Generator ìµœì í™”)
# - AsyncOpenAIë¡œ ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬
# - ê¸°ë³¸: OPENAI_API_KEY(+OPENAI_BASE_URL/FROZEN_OPENAI_BASE_URL)
# - ì—†ìœ¼ë©´ DashScope í˜¸í™˜ í‚¤/ë² ì´ìŠ¤ë¡œ í´ë°±
# =============================================================================
_OPENAI_ASYNC_CLIENT = None
_HAS_OPENAI_ASYNC = False

try:
    from openai import AsyncOpenAI

    _PRIMARY_API_KEY = os.getenv("OPENAI_API_KEY")
    _PRIMARY_BASE_URL = os.getenv("FROZEN_OPENAI_BASE_URL") or os.getenv("OPENAI_BASE_URL")

    _FALLBACK_API_KEY = os.getenv("DASHSCOPE_API_KEY") or os.getenv("DASH_SCOPE_KEY")
    _FALLBACK_BASE_URL = os.getenv(
        "DASHSCOPE_OPENAI_BASE_URL",
        "https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
    )

    _SELECTED_API_KEY = _PRIMARY_API_KEY or _FALLBACK_API_KEY
    _SELECTED_BASE_URL = _PRIMARY_BASE_URL or _FALLBACK_BASE_URL

    if _SELECTED_API_KEY:
        _OPENAI_ASYNC_CLIENT = AsyncOpenAI(
            api_key=_SELECTED_API_KEY,
            base_url=_SELECTED_BASE_URL,
            timeout=60.0,
            max_retries=0,  # ìš°ë¦¬ê°€ ì§ì ‘ ì¬ì‹œë„ ë¡œì§ ê´€ë¦¬
        )
        _HAS_OPENAI_ASYNC = True
        _phase2_logger.info(f"[Phase5] OpenAI AsyncClient initialized: {_SELECTED_BASE_URL}")
except ImportError:
    _phase2_logger.warning("[Phase5] OpenAI SDK not installed. Falling back to DashScope SDK.")
except Exception as e:
    _phase2_logger.warning(f"[Phase5] Failed to initialize OpenAI AsyncClient: {e}")


def _image_to_base64_url(path: str) -> str | None:
    """ì´ë¯¸ì§€ íŒŒì¼ì„ base64 data URLë¡œ ë³€í™˜ (OpenAI Vision API í˜¸í™˜)"""
    if not path or not os.path.exists(path):
        return None

    try:
        # í™•ì¥ìë¡œ MIME íƒ€ì… ê²°ì •
        ext = os.path.splitext(path)[1].lower()
        mime_map = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp',
        }
        mime_type = mime_map.get(ext, 'image/jpeg')

        with open(path, 'rb') as f:
            image_data = f.read()

        base64_data = base64.b64encode(image_data).decode('utf-8')
        return f"data:{mime_type};base64,{base64_data}"
    except Exception:
        return None


async def _call_frozen_generator_async_single(
    client: 'AsyncOpenAI',
    model: str,
    question: str,
    image_paths: List[str],
    max_tokens: int = 1024,
    semaphore: asyncio.Semaphore = None,
) -> Tuple[int, str]:
    """
    OpenAI í˜¸í™˜ APIë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ë‹¨ì¼ í˜¸ì¶œ

    Args:
        client: AsyncOpenAI í´ë¼ì´ì–¸íŠ¸
        model: ëª¨ë¸ëª… (ì˜ˆ: "qwen2.5-vl-72b-instruct")
        question: ì§ˆë¬¸ í…ìŠ¤íŠ¸
        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        semaphore: ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œìš© ì„¸ë§ˆí¬ì–´

    Returns:
        (status_code, answer_text) íŠœí”Œ
    """
    if semaphore:
        async with semaphore:
            return await _call_frozen_generator_async_single_impl(
                client, model, question, image_paths, max_tokens
            )
    else:
        return await _call_frozen_generator_async_single_impl(
            client, model, question, image_paths, max_tokens
        )


async def _call_frozen_generator_async_single_impl(
    client: 'AsyncOpenAI',
    model: str,
    question: str,
    image_paths: List[str],
    max_tokens: int = 1024,
) -> Tuple[int, str]:
    """ë¹„ë™ê¸° í˜¸ì¶œ êµ¬í˜„ë¶€"""
    try:
        qtext = (question or "").strip() or "."

        sys_prompt = (
            "You are a visual QA generator. "
            "Use only the provided images and the user question. "
            "Return ONLY the final answer text without extra explanations. "
            "If you need to crop an image, output exactly one line in the form <bbox>[x1, y1, x2, y2]</bbox>. "
            "BBox rules: pixel coordinates with origin at the top-left (0,0), x increases to the right, y increases downward. "
            "Must satisfy 0 <= x1 < x2 <= image_width and 0 <= y1 < y2 <= image_height; positive area required. "
            "Good example: <bbox>[10, 20, 200, 180]</bbox>. "
            "Bad example (rejected because x1 >= x2): <bbox>[200, 20, 10, 180]</bbox>. "
            "If the bbox is invalid it will be rejected."
        )

        # Responses API í˜•ì‹ìœ¼ë¡œ ì…ë ¥ êµ¬ì„±
        user_content = []

        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì¶”ê°€
        for p in (image_paths or []):
            base64_url = _image_to_base64_url(p)
            if base64_url:
                user_content.append({
                    "type": "input_image",
                    "image_url": base64_url,
                })

        # í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì¶”ê°€
        user_content.append({
            "type": "input_text",
            "text": f"Question: {qtext}"
        })

        inputs = [
            {"role": "developer", "content": sys_prompt},
            {"role": "user", "content": user_content},
        ]

        # ë¹„ë™ê¸° API í˜¸ì¶œ (Reasoning minimal)
        response = await client.responses.create(
            model=model,
            input=inputs,
            reasoning={"effort": "minimal"},
            max_output_tokens=max_tokens,
        )

        # ì‘ë‹µ ì¶”ì¶œ
        answer = getattr(response, "output_text", None)
        if not answer and getattr(response, "output", None):
            # fallback: concatenate message/text parts if output_text absent
            parts = []
            for item in response.output:
                if getattr(item, "type", "") in ("message", "output_text"):
                    if hasattr(item, "content"):
                        for c in getattr(item, "content", []):
                            if c.get("type") == "output_text":
                                parts.append(c.get("text", ""))
                    elif hasattr(item, "text"):
                        parts.append(getattr(item, "text", ""))
            answer = "\n".join([p for p in parts if p])

        return (200, (answer or "").strip())

    except Exception as e:
        error_str = str(e).lower()
        # Rate limit ë˜ëŠ” ì„œë²„ ì˜¤ë¥˜ ê°ì§€
        if "rate" in error_str or "429" in error_str:
            return (429, "")
        elif "500" in error_str or "502" in error_str or "503" in error_str:
            return (503, "")
        elif "timeout" in error_str:
            return (408, "")
        else:
            return (0, "")


# =============================================================================
# [Phase 4] ì´ë¯¸ì§€ ë¡œë”© ìºì‹± (LRU Cache)
# - ë™ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ë°˜ë³µ ë¡œë”© ì‹œ ë””ìŠ¤í¬ I/O íšŒí”¼
# - maxsize=64: ë°°ì¹˜ í¬ê¸° ê³ ë ¤í•œ ìºì‹œ í¬ê¸° (ë©”ëª¨ë¦¬ vs ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„)
# =============================================================================
from functools import lru_cache

@lru_cache(maxsize=64)
def _cached_image_open(path: str) -> 'Image.Image':
    """
    ìºì‹œëœ ì´ë¯¸ì§€ ë¡œë”© í•¨ìˆ˜

    ë™ì¼ ê²½ë¡œì— ëŒ€í•œ ë°˜ë³µ í˜¸ì¶œ ì‹œ ìºì‹œì—ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì£¼ì˜: ë°˜í™˜ëœ ì´ë¯¸ì§€ëŠ” ì›ë³¸ì´ë¯€ë¡œ ìˆ˜ì • ì‹œ .copy() í•„ìš”
    """
    return Image.open(path)


def process_image(image, max_pixels: int = 2048 * 2048, min_pixels: int = 512 * 512):
    import math
    from io import BytesIO
    from PIL import Image

    if isinstance(image, dict):
        image = Image.open(BytesIO(image['bytes']))
    elif isinstance(image, str):
        image = Image.open(image)


    if (image.width * image.height) > max_pixels:
        resize_factor = math.sqrt(max_pixels / (image.width * image.height))
        width, height = int(image.width * resize_factor), int(image.height * resize_factor)
        image = image.resize((width, height))

    if (image.width * image.height) < min_pixels:
        resize_factor = math.sqrt(min_pixels / (image.width * image.height))
        width, height = int(image.width * resize_factor), int(image.height * resize_factor)
        image = image.resize((width, height))

    if image.mode != 'RGB':
        image = image.convert('RGB')

    return image

#ìˆ˜ì • ì¶”ê°€
FORCED_COMPLETION_RESPONSE = "<think>Maximum turn limit reached. Trigger search_complete.</think><search_complete>true</search_complete>"

# =============================================================================
# [Phase 1] ì‚¬ì „ ì»´íŒŒì¼ëœ ì •ê·œì‹ íŒ¨í„´ (ì„±ëŠ¥ ìµœì í™”)
# - ëª¨ë“ˆ ë¡œë“œ ì‹œ 1íšŒë§Œ ì»´íŒŒì¼í•˜ì—¬ ì¬ì‚¬ìš©
# - ë§¤ í˜¸ì¶œë§ˆë‹¤ re.compile() ì˜¤ë²„í—¤ë“œ ì œê±°
# =============================================================================
_RE_EXTRACT_TAGS = re.compile(r"<(search|think|bbox|search_complete)>(.*?)</\1>", re.DOTALL)
_RE_ACTION_PATTERN = re.compile(r'<(search|bbox|search_complete)>(.*?)</\1>', re.DOTALL)
_RE_UID_SUFFIX = re.compile(r'(\d+)$')

# =============================================================================
# [Phase 3] ë£¨í”„ ìµœì í™”ìš© ìƒìˆ˜
# - ë£¨í”„ ë‚´ì—ì„œ ë°˜ë³µ ìƒì„±ë˜ë˜ ë¬¸ìì—´ì„ ëª¨ë“ˆ ë ˆë²¨ ìƒìˆ˜ë¡œ ì¶”ì¶œ
# - ê°€ë…ì„± í–¥ìƒ ë° ìœ ì§€ë³´ìˆ˜ ìš©ì´
# =============================================================================
_MSG_INVALID_BBOX_BASE = (
    "Your previous bbox is invalid.\n"
    "- Expected one line: <bbox>[x1, y1, x2, y2]</bbox>\n"
    "- Pixel coordinates, origin at top-left (0,0), x to the right, y downward\n"
    "- Must satisfy: 0 <= x1 < x2 <= image_width, 0 <= y1 < y2 <= image_height (positive area)\n"
    "- If x1 >= x2 or y1 >= y2, the bbox will be rejected.\n"
)

def _format_invalid_bbox_message(reason: str = "") -> str:
    """Build a user-facing invalid-bbox message with optional reason."""
    reason_line = f"- We rejected your bbox because: {reason}\n" if reason else ""
    return (
        "\n<|im_start|>user\n"
        f"{_MSG_INVALID_BBOX_BASE}"
        f"{reason_line}"
        "Please output a valid bbox now.\n"
        "<|im_end|>\n<|im_start|>assistant\n"
    )

_MSG_INVALID_ACTION = (
    '\n<|im_start|>user\n'
    'Your previous action is invalid. '
    'You must conduct reasoning inside <think> and </think> every time you get new information. '
    'After reasoning, if you find you lack some knowledge, you can call a search engine using <search> query </search> and the user will return the search results. '
    'Whenever you retrieve an image, you may crop it for a clearer view using <bbox>[x1, y1, x2, y2]</bbox>. '
    'You can search as many times as you want. '
    'If you determine that no further external knowledge is needed, you must finish with <search_complete>true</search_compelte>. '
    'Otherwise, continue with <search> or <bbox> actions until you are ready to finish. '
    'Please try again.\n'
    '<|im_end|>\n<|im_start|>assistant\n'
)

# run_llm_loopì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤ ë¦¬ìŠ¤íŠ¸ (ë£¨í”„ ë¶ˆë³€)
_CUT_KEYS = ['input_ids', 'attention_mask', 'position_ids']

@dataclass
class GenerationConfig:
    max_turns: int
    max_prompt_length: int
    num_gpus: int
    search_url: str = None
    #generator added
    crops_dir: str = "./agent_crops"
    frozen_model: str = "qwen2.5-vl-72b-instruct"   # Qwen2.5-VL-72B-Instruct í˜¸í™˜
    frozen_max_tokens: int = 1024
    generator_max_images: int = 8
    use_system_prompt: bool = True
    generator_batch_workers: int = 4
    frozen_max_retries: int = 3
    frozen_backoff_base: float = 1.5
    # [Phase 5] OpenAI ë¹„ë™ê¸° API ì„¤ì •
    frozen_max_concurrent: int = 50          # ë™ì‹œ API ìš”ì²­ ìˆ˜ (ë¹„ë™ê¸° ëª¨ë“œ)
    # [NEW] ê²€ìƒ‰ ìµœì í™” ì˜µì…˜
    async_search: bool = True                # ë¹„ë™ê¸° ë³‘ë ¬ ê²€ìƒ‰ í™œì„±í™”
    search_batch_size: int = 32              # ê²€ìƒ‰ ìš”ì²­ ë°°ì¹˜ í¬ê¸° (512 -> 32ë¡œ ì¤„ì—¬ì„œ ë™ì‹œì„± í™•ë³´)
    search_max_workers: int = 16             # ë³‘ë ¬ ê²€ìƒ‰ ì›Œì»¤ ìˆ˜ (4â†’8ë¡œ ì¦ê°€)
    search_timeout: int = 60                 # ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ (ì´ˆ) - 5ì´ˆâ†’60ì´ˆë¡œ ì¦ê°€
    # [Phase 7] Tool í˜¸ì¶œ ì™„ì „ ë¹„ë™ê¸°í™”
    phase7_tool_async: bool = True           # Phase 7 ë¹„ë™ê¸°í™” í™œì„±í™” (ê¸°ë³¸: True)
    


class LLMGenerationManager:
    def __init__(
        self,
        processor,
        actor_rollout_wg,
        config: GenerationConfig,
        is_validation: bool = False,
        streaming_reward_manager=None,  # [NEW] ìŠ¤íŠ¸ë¦¬ë° Reward Manager
    ):
        self.processor = processor
        self.tokenizer = processor.tokenizer
        self.actor_rollout_wg = actor_rollout_wg
        self.config = config
        self.is_validation = is_validation
        # 0: quiet, 1: ìš”ì•½(ì§ˆë¬¸ ì¼ë¶€/ì´ë¯¸ì§€ ìˆ˜/ë‹µë³€ ì¼ë¶€), 2: ì „ì²´ ìš”ì²­/ì‘ë‹µ
        _frozen_verbose = os.environ.get("FROZEN_DEBUG_VERBOSE", "0")
        try:
            self.verbose_frozen = int(_frozen_verbose)
        except ValueError:
            self.verbose_frozen = 1 if str(_frozen_verbose).lower() in ("1", "true", "t", "yes") else 0

        self.tensor_fn = TensorHelper(TensorConfig(
            pad_token_id=self.tokenizer.pad_token_id
        ))
        #generator added
        os.makedirs(self.config.crops_dir, exist_ok=True)
        os.makedirs("./logs", exist_ok=True)
        self.cropped_images = None
        self.questions = None

        # [NEW] ìŠ¤íŠ¸ë¦¬ë° Reward Manager
        self.streaming_reward_manager = streaming_reward_manager
        self._prompt_completion_status: Dict[str, Dict] = {}

        # [Phase 4] ë¹„ë™ê¸° ì´ë¯¸ì§€ ì €ì¥ì„ ìœ„í•œ ThreadPoolExecutor
        # - ì´ë¯¸ì§€ ì €ì¥ì€ I/O ë°”ìš´ë“œ ì‘ì—…ìœ¼ë¡œ GIL ì˜í–¥ ì ìŒ
        # - max_workers=4: ì¼ë°˜ì ì¸ ë””ìŠ¤í¬ I/O ë³‘ë ¬ ì²˜ë¦¬ì— ì í•©
        self._save_executor = ThreadPoolExecutor(max_workers=4)
        self._pending_saves: List = []  # ì™„ë£Œ ëŒ€ê¸° ì¤‘ì¸ ì €ì¥ ì‘ì—…

        # [ìµœì í™”] HTTP ì—°ê²° í’€ë§ - ì—°ê²° ì¬ì‚¬ìš©ìœ¼ë¡œ 20-30% ì„±ëŠ¥ í–¥ìƒ
        self._search_session = requests.Session()
        # ì—°ê²° í’€ í¬ê¸° ì„¤ì • (ì›Œì»¤ ìˆ˜ì— ë§ì¶¤)
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=self.config.search_max_workers,
            pool_maxsize=self.config.search_max_workers * 2,
            max_retries=0  # ì¬ì‹œë„ëŠ” ìš°ë¦¬ ì½”ë“œì—ì„œ ì²˜ë¦¬
        )
        self._search_session.mount('http://', adapter)
        self._search_session.mount('https://', adapter)

        # [Phase 7 ìµœì í™”] aiohttp ì„¸ì…˜ (Lazy Init)
        self._aio_search_session = None
        self.search_completed = [] # ì´ˆê¸°í™” ì¶”ê°€

        # =========================================================================
        # [Phase 6] ì™„ì „ ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° Reward ì§€ì›
        # - ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ Frozen Generator + Reward ì²˜ë¦¬
        # - ë©”ì¸ ë£¨í”„ ë¸”ë¡œí‚¹ ì—†ì´ GPU 100% í™œìš©
        # =========================================================================
        self._pending_threads: List[threading.Thread] = []  # ì™„ë£Œ ëŒ€ê¸° ì¤‘ì¸ ìŠ¤ë ˆë“œ
        self._thread_lock = threading.Lock()  # Thread-safetyë¥¼ ìœ„í•œ Lock
        self.generated_answers: Dict[int, str] = {}  # ìƒì„±ëœ ë‹µë³€ ì €ì¥ì†Œ
        self._streaming_frozen_generated: set = set()  # ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ ì²˜ë¦¬ ì™„ë£Œëœ ìƒ˜í”Œ ì¸ë±ìŠ¤

        # =========================================================================
        # [Phase 7] Tool í˜¸ì¶œ ì™„ì „ ë¹„ë™ê¸°í™”
        # - Search API í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ì‹œì‘í•˜ê³ , bbox/search_completeì™€ ë³‘ë ¬ ì²˜ë¦¬
        # - Search ê²°ê³¼ëŠ” í•„ìš”í•œ ì‹œì ì—ë§Œ ëŒ€ê¸°
        # - GPU idle ì‹œê°„ ìµœì†Œí™”
        # =========================================================================
        self._tool_executor = ThreadPoolExecutor(
            max_workers=getattr(config, 'search_max_workers', 8),
            thread_name_prefix="ToolAsync"
        )
        self._phase7_enabled = getattr(config, 'phase7_tool_async', True)  # Phase 7 í™œì„±í™” í”Œë˜ê·¸

    def _ensure_saves_complete(self) -> int:
        """
        [Phase 4] ëª¨ë“  ë¹„ë™ê¸° ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ ëŒ€ê¸°

        Returns:
            int: ì™„ë£Œëœ ì €ì¥ ì‘ì—… ìˆ˜
        """
        if not self._pending_saves:
            return 0

        completed = 0
        errors = []
        for future in self._pending_saves:
            try:
                future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                completed += 1
            except Exception as e:
                errors.append(str(e))

        self._pending_saves.clear()

        if errors:
            print(f"[Phase 4] Image save errors ({len(errors)}): {errors[:3]}...")

        return completed

    def _batch_tokenize(self, responses: List[str]) -> torch.Tensor:
        """Tokenize a batch of responses."""
        return self.tokenizer(
            responses, 
            add_special_tokens=False, 
            return_tensors='pt', 
            padding="longest"
        )['input_ids']
    
    def _postprocess_responses_first(self,batch):
        
        responses_str = self.tokenizer.batch_decode(batch.batch['input_ids'], skip_special_tokens=True)
        responses_str = ["<search>"+item.split('Question: ')[1].split(' \n\nassistant\n')[0]+"</search>" for item in responses_str]

        responses = self._batch_tokenize(responses_str)
        return responses, responses_str
        

    def _postprocess_responses(self, responses: torch.Tensor) -> torch.Tensor:
        """Process responses to stop at search operation or answer operation."""
        
        responses_str = self.tokenizer.batch_decode(
            responses, 
            skip_special_tokens=True
        )

        def extract_tags(text):
            # [Phase 1] ì‚¬ì „ ì»´íŒŒì¼ëœ ì •ê·œì‹ ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
            matches = _RE_EXTRACT_TAGS.findall(text)
            result = "\n".join([f"<{tag}>{content}</{tag}>" for tag, content in matches])
            return result

        responses_str = [extract_tags(resp) + self.tokenizer.eos_token for resp in responses_str]

        responses = self._batch_tokenize(responses_str)
        return responses, responses_str

    # def _process_next_obs(self, next_obs: List, rollings) -> torch.Tensor:
    #     """Process next observations from environment."""
    #     next_obs_str = []
    #     multi_modal_data = []
    #     multi_modal_inputs = []
    #     merge_length = self.processor.image_processor.merge_size**2
    #     # print(self.retrievaled_images)
    #     for idx, obs_item in enumerate(next_obs):
    #         # invalid
    #         if isinstance(obs_item,str):
    #             next_obs_str.append(obs_item)
    #             multi_modal_data.append({'image': []})
    #             multi_modal_inputs.append(BatchFeature(dict()))
    #         # invalid
    #         elif isinstance(obs_item, list) and not isinstance(obs_item[0],dict) and len(self.retrievaled_images[idx]) == 0:
    #             next_obs_str.append('\n<|im_start|>user\nYour previous action is invalid. You must conduct reasoning inside <think> and <think> every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine using <search> query </search> and the user will return the search results. Whenever you retrieve an image, you may crop it for a clearer view using <bbox>[x1, y1, x2, y2]</bbox>. You can search as many times as you want. If you determine that no further knowledge is needed, you must finish with <search_complete>true</search_complete>. Otherwise, continue with <search> or <bbox> actions until you are ready to finish. Please try again.\n<|im_end|>\n<|im_start|>assistant\n')
    #             multi_modal_data.append({'image': []})
    #             multi_modal_inputs.append(BatchFeature(dict()))
    #         # crop
    #         elif isinstance(obs_item,list) and not isinstance(obs_item[0],dict):
    #             try:
    #                 latest_image = rollings.non_tensor_batch['multi_modal_data'][idx]['image'][-1]
    #                 width, height = latest_image.size
    #                 raw_images_crop = Image.open(self.retrievaled_images[idx][-1])
    #                 raw_width, raw_height = raw_images_crop.size
    #                 if self.is_validation:
    #                     obs_item = [obs_item[0]-28, obs_item[1]-28, obs_item[2]+28, obs_item[3]+28]
    #                 crop_area = [int(raw_width * obs_item[0] / width), int(raw_height * obs_item[1] / height), int(raw_width * obs_item[2] / width), int(raw_height * obs_item[3] / height)]
    #                 crop_area = [max(0, crop_area[0]), max(0, crop_area[1]), min(raw_width, crop_area[2]), min(raw_height, crop_area[3])]
    #                 input_images_list = [raw_images_crop.crop((crop_area[0], crop_area[1], crop_area[2], crop_area[3]))]
    #                 raw_images_list = [process_image(image, 512*28*28, 256*28*28) for image in input_images_list]

    #                 #generator added
    #                 crop_path = os.path.join(self.config.crops_dir, f"{uuid.uuid4().hex}.jpg")
    #                 raw_images_list[0].save(crop_path)
    #                 self.cropped_images[idx].append(crop_path)
    #                 #                    

    #                 multi_modal_data.append({'image': raw_images_list})
    #                 image_inputs = self.processor.image_processor(raw_images_list, return_tensors='pt') 
    #                 multi_modal_inputs.append(image_inputs)
    #                 image_grid_thw = image_inputs['image_grid_thw']
    #                 obs_str = ''.join([f"<|vision_start|>{self.processor.image_token * (image_grid_thw_item.prod() // merge_length)}<|vision_end|>" for image_grid_thw_item in image_grid_thw])
    #                 raw_obs_str = f"<|vision_start|>{self.processor.image_token}<|vision_end|>" * len(image_grid_thw) 
    #                 obs_str = '\n<|im_start|>user\n' + obs_str + '<|im_end|>\n<|im_start|>assistant\n'
    #                 next_obs_str.append(obs_str)   
    #             except Exception as e:
    #                 next_obs_str.append('\n<|im_start|>user\nYour previous action is invalid. You must conduct reasoning inside <think> and </think> every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine using <search> query </search> and the user will return the search results. Whenever you retrieve an image, you may crop it for a clearer view using <bbox>[x1, y1, x2, y2]</bbox>. You can search as many times as you want. If you determine that no further external knowledge is needed, you must finish with <search_complete>true</search_complete>. Otherwise, continue with <search> or <bbox> actions until you are ready to finish. Please try again.\n<|im_end|>\n<|im_start|>assistant\n')
    #                 multi_modal_data.append({'image': []})
    #                 multi_modal_inputs.append(BatchFeature(dict())) 
    #         # ret image
    #         elif isinstance(obs_item,list) and isinstance(obs_item[0],dict):

    #             img_file_list = [item['image_file'] for item in obs_item]
    #             for image_item in img_file_list:
    #                 if image_item not in self.retrievaled_images[idx]:
    #                     self.retrievaled_images[idx].append(image_item)
    #                     # input_images_list = img_file_list[:1]
    #                     input_images_list = [image_item]
    #                     break
    #             #ìˆ˜ì • pixe_value
    #             # raw_images_list = [process_image(image, 512*28*28, 256*28*28) for image in input_images_list]

    #             # multi_modal_data.append({'image': raw_images_list})
    #             # image_inputs = self.processor.image_processor(raw_images_list, return_tensors='pt')

    #             # multi_modal_inputs.append(image_inputs)
    #             # image_grid_thw = image_inputs['image_grid_thw']

    #             # obs_str = ''.join([f"<|vision_start|>{self.processor.image_token * (image_grid_thw_item.prod() // merge_length)}<|vision_end|>" for image_grid_thw_item in image_grid_thw])
    #             # raw_obs_str = f"<|vision_start|>{self.processor.image_token}<|vision_end|>" * len(image_grid_thw) 
    #             # obs_str = '\n<|im_start|>user\n' + obs_str + '<|im_end|>\n<|im_start|>assistant\n'
    #             # next_obs_str.append(obs_str)
    #             raw_images_list = [process_image(image, 512*28*28, 256*28*28) for image in input_images_list]
    #             image_inputs = self.processor.image_processor(raw_images_list, return_tensors='pt')

    #             if 'pixel_values' in image_inputs:
    #                 # ì •ìƒì¸ ê²½ìš°: ê¸°ì¡´ ë¡œì§ ìˆ˜í–‰
    #                 multi_modal_data.append({'image': raw_images_list})
    #                 multi_modal_inputs.append(image_inputs)
                    
    #                 image_grid_thw = image_inputs['image_grid_thw']
    #                 obs_str = ''.join([f"<|vision_start|>{self.processor.image_token * (image_grid_thw_item.prod() // merge_length)}<|vision_end|>" for image_grid_thw_item in image_grid_thw])
    #                 obs_str = '\n<|im_start|>user\n' + obs_str + '<|im_end|>\n<|im_start|>assistant\n'
    #                 next_obs_str.append(obs_str)
    #             else:
    #                 # ì‹¤íŒ¨í•œ ê²½ìš°: System Error ë©”ì‹œì§€ ì‚½ì… ë° ë¹ˆ ë°ì´í„° ì¶”ê°€
    #                 print(f"Warning: Image processing failed for idx {idx}. Inserting Error Message.")
    #                 multi_modal_data.append({'image': []})
    #                 multi_modal_inputs.append(BatchFeature(dict()))
                    
    #                 error_msg = "\n<|im_start|>user\nSystem Error: Failed to load image due to format issues.\n<|im_end|>\n<|im_start|>assistant\n"
    #                 next_obs_str.append(error_msg)
    #             #//
    #         else:
    #             raise ValueError('invalid observation')
    
    
        # next_obs_ids = self.tokenizer(
        #     next_obs_str, 
        #     padding='longest',
        #     return_tensors='pt',
        #     add_special_tokens=False,  # Prevents adding special tokens
        # )['input_ids']

        # return next_obs_ids, next_obs_str, multi_modal_data, multi_modal_inputs
    def _process_next_obs(self, next_obs: List, rollings) -> torch.Tensor:
        """
        [Phase 8] Parallel BBox Processing
        BBox Crop ë° Image Processingì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ ì†ë„ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
        """
        start_time = _time.perf_counter()
        batch_size = len(next_obs)
        
        # [Safety] Check batch size consistency
        if self.retrievaled_images is None:
             # Should not happen if initialized correctly
             self.retrievaled_images = [[] for _ in range(batch_size)]
             
        total_size = len(self.retrievaled_images)
        if batch_size != total_size:
            print(f"ğŸš¨ [CRITICAL] Batch size mismatch in _process_next_obs: next_obs({batch_size}) vs retrievaled_images({total_size})")
            if batch_size > total_size:
                print("  -> Truncating next_obs to match retrievaled_images size.")
                next_obs = next_obs[:total_size]
                batch_size = total_size
            else:
                print("  -> next_obs is smaller. Proceeding with caution.")

        next_obs_str = [None] * batch_size
        multi_modal_data = [None] * batch_size
        multi_modal_inputs = [None] * batch_size
        
        merge_length = self.processor.image_processor.merge_size**2
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ Future ë¦¬ìŠ¤íŠ¸
        bbox_futures = {} # {future: idx}

        # 1. Main Loop: Search ì²˜ë¦¬ ë° BBox ì‘ì—… ìŠ¤ì¼€ì¤„ë§
        for idx, obs_item in enumerate(next_obs):
            # [Safety] Index boundary check
            if idx >= len(self.retrievaled_images):
                print(f"[Error] Index {idx} out of bounds for retrievaled_images (len={len(self.retrievaled_images)}). Skipping.")
                next_obs_str[idx] = ""
                multi_modal_data[idx] = {'image': []}
                multi_modal_inputs[idx] = BatchFeature(dict())
                continue

            # 1. Invalid String
            if isinstance(obs_item, str):
                next_obs_str[idx] = obs_item
                multi_modal_data[idx] = {'image': []}
                multi_modal_inputs[idx] = BatchFeature(dict())
                continue

            # 2. Invalid Action (No previous image)
            if isinstance(obs_item, list) and not isinstance(obs_item[0], dict) and len(self.retrievaled_images[idx]) == 0:
                next_obs_str[idx] = '\n<|im_start|>user\nInvalid action: No image to crop. Please search first.\n<|im_end|>\n<|im_start|>assistant\n'
                multi_modal_data[idx] = {'image': []}
                multi_modal_inputs[idx] = BatchFeature(dict())
                continue

            # 3. BBox / Crop (Schedule for Parallel Execution)
            if isinstance(obs_item, list) and not isinstance(obs_item[0], dict):
                try:
                    # í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ (ìŠ¤ë ˆë“œ ì•ˆì „í•˜ê²Œ)
                    latest_image = rollings.non_tensor_batch['multi_modal_data'][idx]['image'][-1]
                    last_img_path = self.retrievaled_images[idx][-1]
                    
                    # Future ì œì¶œ
                    future = self._tool_executor.submit(
                        self._process_bbox_single,
                        idx, obs_item, latest_image.size, last_img_path, self.is_validation
                    )
                    bbox_futures[future] = idx
                except Exception as e:
                    print(f"[ProcessObs] BBox schedule error at {idx}: {e}")
                    next_obs_str[idx] = '\n<|im_start|>user\n[System Error: BBox Scheduling Failed]\n<|im_end|>\n<|im_start|>assistant\n'
                    multi_modal_data[idx] = {'image': []}
                    multi_modal_inputs[idx] = BatchFeature(dict())

            # 4. Search / Retrieval (Process Immediately due to State Dependency)
            elif isinstance(obs_item, list) and isinstance(obs_item[0], dict):
                try:
                    img_file_list = [item['image_file'] for item in obs_item]
                    input_images_list = []
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸ (retrievaled_images)
                    for image_item in img_file_list:
                        if image_item not in self.retrievaled_images[idx]:
                            self.retrievaled_images[idx].append(image_item)
                            input_images_list = [image_item]
                            break
                    
                    if not input_images_list:
                        # ì´ë¯¸ ìˆëŠ” ì´ë¯¸ì§€ì´ê±°ë‚˜ ì—†ëŠ” ê²½ìš°? (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                        # ê¸°ì¡´ ë¡œì§ì€ input_images_listê°€ ë¹„ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìŒ. ë°©ì–´ ì½”ë“œ ì¶”ê°€.
                        if len(img_file_list) > 0:
                             input_images_list = [img_file_list[0]] # Fallback to first
                        else:
                             raise ValueError("No images returned from search")

                    raw_images_list = [process_image(image, 512*28*28, 256*28*28) for image in input_images_list]
                    image_inputs = self.processor.image_processor(raw_images_list, return_tensors='pt')

                    if 'pixel_values' in image_inputs:
                        multi_modal_data[idx] = {'image': raw_images_list}
                        multi_modal_inputs[idx] = image_inputs
                        
                        image_grid_thw = image_inputs['image_grid_thw']
                        obs_str = ''.join([f"<|vision_start|>{self.processor.image_token * (image_grid_thw_item.prod() // merge_length)}<|vision_end|>" for image_grid_thw_item in image_grid_thw])
                        next_obs_str[idx] = '\n<|im_start|>user\n' + obs_str + '<|im_end|>\n<|im_start|>assistant\n'
                    else:
                        raise ValueError("No pixel_values in search result")

                except Exception as e:
                    # print(f"[DEBUG] Search Error at idx {idx}: {e}")
                    next_obs_str[idx] = "\n<|im_start|>user\n[System Error: Search Image Failed]\n<|im_end|>\n<|im_start|>assistant\n"
                    multi_modal_data[idx] = {'image': []}
                    multi_modal_inputs[idx] = BatchFeature(dict())
            
            else:
                raise ValueError(f'Invalid observation at idx {idx}')

        if bbox_futures:
            _phase2_logger.info(f"[Tool:BBox] Scheduled {len(bbox_futures)} parallel crop tasks.")

        # 2. Collect Parallel BBox Results
        if bbox_futures:
            for future in as_completed(bbox_futures):
                idx = bbox_futures[future]
                try:
                    result = future.result()
                    
                    # ë¹„ë™ê¸° ì €ì¥ ìš”ì²­ (ê²°ê³¼ ì²˜ë¦¬ì™€ ë³„ê°œë¡œ)
                    if 'img_to_save' in result and 'crop_path' in result:
                        self.cropped_images[idx].append(result['crop_path'])
                        self._save_executor.submit(result['img_to_save'].save, result['crop_path'])

                    multi_modal_data[idx] = {'image': result['raw_images_list']}
                    multi_modal_inputs[idx] = result['image_inputs']
                    next_obs_str[idx] = result['obs_str']

                except Exception as e:
                    # ì§§ì€ ìš”ì•½ + í•„ìš” ì‹œ ìƒì„¸ ë¡œê·¸
                    bbox_verbose = os.environ.get("BBOX_DEBUG_VERBOSE", "0") in ("1", "true", "True")
                    msg = f"[Tool:BBox] Worker Error at idx {idx}: {e}"
                    if bbox_verbose:
                        _phase2_logger.error(msg)
                    else:
                        _phase2_logger.warning(msg)
                    next_obs_str[idx] = _format_invalid_bbox_message(str(e))
                    multi_modal_data[idx] = {'image': []}
                    multi_modal_inputs[idx] = BatchFeature(dict())
            
            _phase2_logger.info(f"[Tool:BBox] All tasks completed in {_time.perf_counter() - start_time:.3f}s")

        # 3. Finalize: Noneì¸ í•­ëª© ì±„ìš°ê¸° (ì•ˆì „ì¥ì¹˜)
        for i in range(batch_size):
            if next_obs_str[i] is None:
                next_obs_str[i] = ""
            if multi_modal_data[i] is None:
                multi_modal_data[i] = {'image': []}
            if multi_modal_inputs[i] is None:
                multi_modal_inputs[i] = BatchFeature(dict())

        # 4. Tokenize
        next_obs_ids = self.tokenizer(
            next_obs_str, 
            padding='longest',
            return_tensors='pt',
            add_special_tokens=False,
        )['input_ids']

        return next_obs_ids, next_obs_str, multi_modal_data, multi_modal_inputs

    def _process_bbox_single(self, idx, obs_item, size, last_img_path, is_validation):
        """Worker function for BBox processing"""
        start_time = _time.perf_counter()
        # _phase2_logger.debug(f"[Tool:BBox] Idx {idx}: Start processing. Coords={obs_item}, Image={last_img_path}")

        width, height = size
        bbox_verbose = os.environ.get("BBOX_DEBUG_VERBOSE", "0") in ("1", "true", "True")
        
        # [Safety] Prevent ZeroDivisionError
        if width <= 0 or height <= 0:
            raise ValueError(f"Invalid image size: {width}x{height}")

        # IO: Load Image (Cached)
        raw_images_crop = _cached_image_open(last_img_path)
        raw_width, raw_height = raw_images_crop.size

        # Validation Adjustment
        if is_validation:
            obs_item = [obs_item[0]-28, obs_item[1]-28, obs_item[2]+28, obs_item[3]+28]
        
        # CPU: Calculate Crop
        crop_area = [int(raw_width * obs_item[0] / width), int(raw_height * obs_item[1] / height), int(raw_width * obs_item[2] / width), int(raw_height * obs_item[3] / height)]
        crop_area = [max(0, crop_area[0]), max(0, crop_area[1]), min(raw_width, crop_area[2]), min(raw_height, crop_area[3])]
        
        # CPU: Perform Crop
        try:
            input_images_list = [raw_images_crop.crop((crop_area[0], crop_area[1], crop_area[2], crop_area[3]))]
        except Exception as e:
            msg = f"Crop failed (idx={idx}): {e}"
            if bbox_verbose:
                _phase2_logger.error(msg)
            else:
                _phase2_logger.warning(msg)
            raise
        
        # CPU: Resize (Heavy)
        raw_images_list = [process_image(image, 512*28*28, 256*28*28) for image in input_images_list]

        # Async Save Prep
        crop_path = os.path.join(self.config.crops_dir, f"{uuid.uuid4().hex}.jpg")
        img_to_save = raw_images_list[0].copy()

        # CPU: Image Processor (Heavy)
        image_inputs = self.processor.image_processor(raw_images_list, return_tensors='pt')

        if 'pixel_values' not in image_inputs:
            raise ValueError("BBox processing produced no pixel_values")

        # Result Formatting
        # [Test Support] Handle MagicMock or missing merge_size
        merge_size = getattr(self.processor.image_processor, 'merge_size', 2)
        try:
            merge_size = int(merge_size)
        except:
            merge_size = 2
            
        merge_length = merge_size**2
        if merge_length <= 0:
            merge_length = 4 # Fallback default

        image_grid_thw = image_inputs['image_grid_thw']
        obs_str = ''.join([f"<|vision_start|>{self.processor.image_token * (image_grid_thw_item.prod() // merge_length)}<|vision_end|>" for image_grid_thw_item in image_grid_thw])
        obs_str = '\n<|im_start|>user\n' + obs_str + '<|im_end|>\n<|im_start|>assistant\n'

        duration = _time.perf_counter() - start_time
        # _phase2_logger.debug(f"[Tool:BBox] Idx {idx}: Completed in {duration:.4f}s")

        return {
            'raw_images_list': raw_images_list,
            'image_inputs': image_inputs,
            'obs_str': obs_str,
            'crop_path': crop_path,
            'img_to_save': img_to_save
        }

#//

    # def _concat_multi_modal_data(self, rollings, next_obs_multi_modal_data:list, next_obs_multi_modal_inputs:list):
    #     if not 'multi_modal_inputs' in rollings.non_tensor_batch.keys():

    #         rollings.non_tensor_batch['multi_modal_inputs'] = np.empty(len(next_obs_multi_modal_data), dtype=object)
    #         for idx, item in enumerate(next_obs_multi_modal_inputs):
    #             rollings.non_tensor_batch['multi_modal_inputs'][idx] = item

    #         rollings.non_tensor_batch['multi_modal_data'] = np.array(next_obs_multi_modal_data, dtype=object)

    #     else:
    #         for idx, multi_modal_data_item in enumerate(next_obs_multi_modal_data):
    #             if len(multi_modal_data_item['image']) > 0:
    #                 # data
    #                 #ìˆ˜ì • pixel_value
    #                 # rollings.non_tensor_batch['multi_modal_data'][idx]['image'].extend(multi_modal_data_item['image'])
    #                 # if 'pixel_values' in rollings.non_tensor_batch['multi_modal_inputs'][idx]:
    #                 #     rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'] = torch.cat((rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'], next_obs_multi_modal_inputs[idx]['pixel_values']),dim=0)
    #                 #     rollings.non_tensor_batch['multi_modal_inputs'][idx]['image_grid_thw'] = torch.cat((rollings.non_tensor_batch['multi_modal_inputs'][idx]['image_grid_thw'], next_obs_multi_modal_inputs[idx]['image_grid_thw']),dim=0)
    #                 # else:
    #                 #     rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'] = next_obs_multi_modal_inputs[idx]['pixel_values']
    #                 #     rollings.non_tensor_batch['multi_modal_inputs'][idx]['image_grid_thw'] = next_obs_multi_modal_inputs[idx]['image_grid_thw']
    #                 # â–¼â–¼â–¼ [ìˆ˜ì • í•µì‹¬] ì´ë¯¸ì§€ê°€ ìˆë‹¤ê³  í•´ë„, ì‹¤ì œ í…ì„œ í‚¤(pixel_values)ê°€ ìˆëŠ”ì§€ í•œ ë²ˆ ë” í™•ì¸í•´ì•¼ í•¨ â–¼â–¼â–¼
    #                 rollings.non_tensor_batch['multi_modal_data'][idx]['image'].extend(multi_modal_data_item['image'])
                    
    #                 # [í•µì‹¬ ìˆ˜ì •] pixel_valuesê°€ ìˆìœ¼ë©´ ì •ìƒ ë³‘í•©, ì—†ìœ¼ë©´ "ë”ë¯¸ ë°ì´í„°" ìƒì„±í•˜ì—¬ ë³‘í•©
    #                 if 'pixel_values' in next_obs_multi_modal_inputs[idx]:
    #                     # A. ì •ìƒ ì¼€ì´ìŠ¤
    #                     if 'pixel_values' in rollings.non_tensor_batch['multi_modal_inputs'][idx]:
    #                         rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'] = torch.cat((rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'], next_obs_multi_modal_inputs[idx]['pixel_values']),dim=0)
    #                         rollings.non_tensor_batch['multi_modal_inputs'][idx]['image_grid_thw'] = torch.cat((rollings.non_tensor_batch['multi_modal_inputs'][idx]['image_grid_thw'], next_obs_multi_modal_inputs[idx]['image_grid_thw']),dim=0)
    #                     else:
    #                         rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'] = next_obs_multi_modal_inputs[idx]['pixel_values']
    #                         rollings.non_tensor_batch['multi_modal_inputs'][idx]['image_grid_thw'] = next_obs_multi_modal_inputs[idx]['image_grid_thw']
                    
    #                 else:
    #                     # B. ë¹„ì •ìƒ ì¼€ì´ìŠ¤ (í† í°ì€ ìˆëŠ”ë° í”½ì…€ê°’ì´ ì‚¬ë¼ì§) -> ë”ë¯¸ ë°ì´í„° ì£¼ì…í•˜ì—¬ ì§ ë§ì¶¤
    #                     print(f"Warning: 'pixel_values' missing at idx {idx} in _concat. Using Dummy Black Image to prevent IndexError.")
                        
    #                     # Qwen2-VL ê¸°ì¤€ ë”ë¯¸ ë°ì´í„° ìƒì„± (1x1 í”½ì…€)
    #                     # pixel_values: ëŒ€ëµì ì¸ shapeê³¼ íƒ€ì…ë§Œ ë§ì¶”ë©´ ë¨
    #                     dummy_pixel_values = torch.zeros((1, 1176), dtype=torch.float32).to(rollings.batch['input_ids'].device) # 1176 is minimal flattened size roughly
                        
    #                     # image_grid_thw: [1, h, w] -> [1, 1, 1] (ì‹œê°„1, ë†’ì´1, ë„ˆë¹„1)
    #                     dummy_grid = torch.tensor([[1, 1, 1]], dtype=torch.long).to(rollings.batch['input_ids'].device)

    #                     if 'pixel_values' in rollings.non_tensor_batch['multi_modal_inputs'][idx]:
    #                         # ê¸°ì¡´ í…ì„œì™€ ëª¨ì–‘(Shape)ì„ ë§ì¶°ì„œ ë³‘í•© ì‹œë„ (ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ unsqueeze ë“± ì²˜ë¦¬)
    #                         try:
    #                             # ê¸°ì¡´ pixel_valuesì˜ feature dimension(ë§ˆì§€ë§‰ ì°¨ì›)ì„ í™•ì¸í•˜ì—¬ ë§ì¶¤
    #                             expected_dim = rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'].shape[-1]
    #                             if dummy_pixel_values.shape[-1] != expected_dim:
    #                                  dummy_pixel_values = torch.zeros((1, expected_dim), dtype=torch.float32).to(rollings.batch['input_ids'].device)

    #                             rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'] = torch.cat((
    #                                 rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'], 
    #                                 dummy_pixel_values
    #                             ), dim=0)
    #                             rollings.non_tensor_batch['multi_modal_inputs'][idx]['image_grid_thw'] = torch.cat((
    #                                 rollings.non_tensor_batch['multi_modal_inputs'][idx]['image_grid_thw'], 
    #                                 dummy_grid
    #                             ), dim=0)
    #                         except Exception as e:
    #                             print(f"Error merging dummy tensor: {e}. Skipping (Crash risk high).")
    #                     else:
    #                         # ì´ˆê¸° í• ë‹¹
    #                         rollings.non_tensor_batch['multi_modal_inputs'][idx]['pixel_values'] = dummy_pixel_values
    #                         rollings.non_tensor_batch['multi_modal_inputs'][idx]['image_grid_thw'] = dummy_grid
    #                 # â–²â–²â–² [ìˆ˜ì • ë] â–²â–²â–²
    #     return rollings
    def _concat_multi_modal_data(self, rollings, next_obs_multi_modal_data: list, next_obs_multi_modal_inputs: list):
        """
        [Phase 2 ìµœì í™”] ë¡¤ë§ ìƒíƒœì— ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.

        ìµœì í™” ë‚´ìš©:
        - ë°˜ë³µì ì¸ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ì„ ë¡œì»¬ ë³€ìˆ˜ë¡œ ìºì‹±
        - ì¡°ê±´ë¬¸ êµ¬ì¡° ê°œì„ ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ì²´í¬ ê°ì†Œ
        - ì„±ëŠ¥ ë¡œê¹…: Phase2PerfTimerë¡œ ì¸¡ì • ê°€ëŠ¥ (PHASE2_PERF_LOG=1)
        """
        with Phase2PerfTimer("_concat_multi_modal_data", batch_size=len(next_obs_multi_modal_data)):

            # [Phase 2] ìì£¼ ì ‘ê·¼í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë¡œì»¬ ë³€ìˆ˜ë¡œ ìºì‹±
            non_tensor_batch = rollings.non_tensor_batch

            if 'multi_modal_inputs' not in non_tensor_batch:
                # ì´ˆê¸°í™” ì¼€ì´ìŠ¤
                non_tensor_batch['multi_modal_inputs'] = np.empty(len(next_obs_multi_modal_data), dtype=object)
                for idx, item in enumerate(next_obs_multi_modal_inputs):
                    non_tensor_batch['multi_modal_inputs'][idx] = item

                non_tensor_batch['multi_modal_data'] = np.array(next_obs_multi_modal_data, dtype=object)

            else:
                # [Phase 2] ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¡œì»¬ ë³€ìˆ˜ë¡œ ìºì‹±
                existing_multi_modal_data = non_tensor_batch['multi_modal_data']
                existing_multi_modal_inputs = non_tensor_batch['multi_modal_inputs']

                for idx, multi_modal_data_item in enumerate(next_obs_multi_modal_data):
                    # [Phase 2] ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ (ë¶ˆí•„ìš”í•œ ì²´í¬ íšŒí”¼)
                    if len(multi_modal_data_item['image']) == 0:
                        continue

                    new_inputs = next_obs_multi_modal_inputs[idx]

                    # ë°©ì–´ ë¡œì§: pixel_valuesê°€ ìˆì„ ë•Œë§Œ ë³‘í•©
                    if 'pixel_values' not in new_inputs:
                        continue

                    # [Phase 2] ë¡œì»¬ ë³€ìˆ˜ ìºì‹±ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ íšŸìˆ˜ ê°ì†Œ
                    existing_inputs = existing_multi_modal_inputs[idx]
                    existing_data = existing_multi_modal_data[idx]

                    # ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ í™•ì¥
                    existing_data['image'].extend(multi_modal_data_item['image'])

                    # í…ì„œ ì—°ê²°
                    if 'pixel_values' in existing_inputs:
                        # [Phase 2] ê¸°ì¡´ í…ì„œì™€ ìƒˆ í…ì„œ ì—°ê²°
                        existing_inputs['pixel_values'] = torch.cat(
                            (existing_inputs['pixel_values'], new_inputs['pixel_values']),
                            dim=0
                        )
                        existing_inputs['image_grid_thw'] = torch.cat(
                            (existing_inputs['image_grid_thw'], new_inputs['image_grid_thw']),
                            dim=0
                        )
                    else:
                        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì¸ ê²½ìš° ì§ì ‘ í• ë‹¹
                        existing_inputs['pixel_values'] = new_inputs['pixel_values']
                        existing_inputs['image_grid_thw'] = new_inputs['image_grid_thw']

            return rollings
#//

    def _update_rolling_state(self, rollings, cur_responses: torch.Tensor, 
                            next_obs_ids: torch.Tensor) -> Dict:
        """Update rolling state with new responses and observations."""
        # Concatenate and handle padding
        if next_obs_ids.shape[1] != 0:
            new_input_ids = self.tensor_fn.concatenate_with_padding([
                rollings.batch['input_ids'],
                cur_responses,
                next_obs_ids
            ])
        else:
            new_input_ids = self.tensor_fn.concatenate_with_padding([
                rollings.batch['input_ids'],
                cur_responses
            ])
        # Create attention mask and position ids
        new_attention_mask = self.tensor_fn.create_attention_mask(new_input_ids)
        new_position_ids = self.tensor_fn.create_position_ids(new_attention_mask)

        # Cut to appropriate length
        effective_len = new_attention_mask.sum(dim=1).max()
        max_len = min(self.config.max_prompt_length, effective_len)
        
        return DataProto.from_dict({
            'input_ids': new_input_ids[:, -max_len:],
            'position_ids': new_position_ids[:, -max_len:],
            'attention_mask': new_attention_mask[:, -max_len:]
        }, rollings.non_tensor_batch)

    def _update_right_side(self, right_side: Dict, 
                          cur_responses: torch.Tensor,
                          next_obs_ids: torch.Tensor = None) -> Dict:
        """Update right side state."""
        if next_obs_ids != None and next_obs_ids.shape[1] != 0:
            responses = self.tensor_fn.concatenate_with_padding([
                right_side['responses'],
                cur_responses,
                next_obs_ids
            ], pad_to_left=False)
        else:
            responses = self.tensor_fn.concatenate_with_padding([
                right_side['responses'],
                cur_responses,
            ], pad_to_left=False)
        
        effective_len = self.tensor_fn.create_attention_mask(responses).sum(dim=1).max()
        max_len = min(self.config.max_prompt_length, effective_len)
        
        return {'responses': responses[:, :max_len]}


    def _generate_with_gpu_padding(self, active_batch: DataProto) -> DataProto:
        """
        [Phase 2 ìµœì í™”] Wrapper for generation that handles multi-GPU padding requirements.

        ìµœì í™” ë‚´ìš©:
        - í…ì„œ ì—°ì‚° í†µí•©: 3íšŒ torch.cat â†’ ë” íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ ê°œì„ 
        - non_tensor_batch ì²˜ë¦¬: ì¤‘ì²© ë£¨í”„ ì œê±°, ë¦¬ìŠ¤íŠ¸ ê³±ì…ˆ í™œìš©
        - ì„±ëŠ¥ ë¡œê¹…: Phase2PerfTimerë¡œ ì¸¡ì • ê°€ëŠ¥ (PHASE2_PERF_LOG=1)
        """
        with Phase2PerfTimer("_generate_with_gpu_padding",
                            batch_size=active_batch.batch['input_ids'].shape[0] if active_batch.batch is not None else None):

            num_gpus = self.config.num_gpus
            if num_gpus <= 1:
                return self.actor_rollout_wg.generate_sequences(active_batch)

            batch_size = active_batch.batch['input_ids'].shape[0]
            remainder = batch_size % num_gpus

            if remainder == 0:
                return self.actor_rollout_wg.generate_sequences(active_batch)

            # Add padding sequences
            padding_size = num_gpus - remainder

            # [Phase 2] íŒ¨ë”© ID í† í¬ë‚˜ì´ì§• (1íšŒë§Œ ìˆ˜í–‰)
            padded_ids = self.tokenizer(
                ['<|im_start|>user\nHi, who are u?<|im_end|>\n<|im_start|>assistant\n'],
                padding='longest',
                return_tensors='pt',
                add_special_tokens=False,
            )['input_ids'][0]  # ë°”ë¡œ [0] ì¸ë±ì‹±

            # [Phase 2] íŒ¨ë”© í…ì„œ ìƒì„±
            pad_input_ids = torch.full_like(active_batch.batch['input_ids'][0], 151643, dtype=torch.int64)
            pad_input_ids[:len(padded_ids)] = padded_ids
            pad_attention_mask = self.tensor_fn.create_attention_mask(pad_input_ids)
            pad_input_ids = pad_input_ids.unsqueeze(0)
            pad_attention_mask = pad_attention_mask.unsqueeze(0)
            pad_position_ids = self.tensor_fn.create_position_ids(pad_attention_mask)

            # [Phase 2] í…ì„œ ë°°ì¹˜ êµ¬ì„± - ë°˜ë³µ íšŸìˆ˜ ì‚¬ì „ ê³„ì‚°
            # repeat ì¸ìë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ì¬ì‚¬ìš©
            repeat_dims_2d = (padding_size, 1)  # 2D í…ì„œìš©

            padded_batch = {
                'attention_mask': torch.cat([
                    active_batch.batch['attention_mask'],
                    pad_attention_mask.repeat(*repeat_dims_2d)
                ], dim=0),
                'input_ids': torch.cat([
                    active_batch.batch['input_ids'],
                    pad_input_ids.repeat(*repeat_dims_2d)
                ], dim=0),
                'position_ids': torch.cat([
                    active_batch.batch['position_ids'],
                    pad_position_ids.repeat(*repeat_dims_2d)
                ], dim=0),
            }

            # [Phase 2] Non-tensor batch ì²˜ë¦¬ ìµœì í™”
            # - ì¤‘ì²© ë£¨í”„ ì œê±°: ë¦¬ìŠ¤íŠ¸ ê³±ì…ˆìœ¼ë¡œ ëŒ€ì²´
            # - ê° í‚¤ë³„ ì²˜ë¦¬ë¥¼ ì¸ë¼ì¸í™”
            padded_non_tensor_batch = {}
            list_ids = padded_ids.tolist()  # 1íšŒë§Œ ë³€í™˜

            for k, v in active_batch.non_tensor_batch.items():
                if k == 'raw_prompt_ids':
                    # ë¦¬ìŠ¤íŠ¸ ê³±ì…ˆìœ¼ë¡œ ë™ì¼í•œ ê°ì²´ ì°¸ì¡° (ë©”ëª¨ë¦¬ íš¨ìœ¨)
                    pad_items = [list_ids] * padding_size
                elif k == 'multi_modal_inputs':
                    # ê°ê° ìƒˆë¡œìš´ ë”•ì…”ë„ˆë¦¬ ìƒì„± (mutable ê°ì²´ì´ë¯€ë¡œ)
                    pad_items = [{} for _ in range(padding_size)]
                elif k == 'multi_modal_data':
                    pad_items = [{'image': []} for _ in range(padding_size)]
                else:
                    # ì•Œ ìˆ˜ ì—†ëŠ” í‚¤: Noneìœ¼ë¡œ íŒ¨ë”©
                    pad_items = [None] * padding_size

                # [Fix] ì°¨ì› ë¶ˆì¼ì¹˜ ë°©ì§€: ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ concatenate
                # vê°€ ë‹¤ì°¨ì›ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ tolist()ë¡œ ë³€í™˜ í›„ í•©ì¹¨
                if isinstance(v, np.ndarray):
                    combined = list(v) + pad_items
                else:
                    combined = list(v) + pad_items
                padded_non_tensor_batch[k] = np.array(combined, dtype=object)

            padded_active_batch = DataProto.from_dict(padded_batch, padded_non_tensor_batch)

            # Generate with padded batch
            padded_output = self.actor_rollout_wg.generate_sequences(padded_active_batch)

            # [Phase 2] Remove padding from output - ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜ ìœ ì§€ (ì´ë¯¸ íš¨ìœ¨ì )
            trimmed_batch = {k: v[:-padding_size] for k, v in padded_output.batch.items()}

            # Handle meta_info if present
            if hasattr(padded_output, 'meta_info') and padded_output.meta_info:
                trimmed_meta = {}
                for k, v in padded_output.meta_info.items():
                    if isinstance(v, torch.Tensor):
                        trimmed_meta[k] = v[:-padding_size]
                    else:
                        trimmed_meta[k] = v
                padded_output.meta_info = trimmed_meta

            padded_output.batch = trimmed_batch
            return padded_output

    def _raw_prompt_ids(self, rollings):
        """
        [Phase 2 ìµœì í™”] ë¡¤ë§ ìƒíƒœì˜ input_idsì—ì„œ ì—°ì†ëœ ì´ë¯¸ì§€ í† í°(151655)ì„ ì••ì¶•í•©ë‹ˆë‹¤.

        ìµœì í™” ë‚´ìš©:
        - ë‚´ë¶€ í•¨ìˆ˜ë¥¼ ë©”ì„œë“œ ë ˆë²¨ë¡œ ì´ë™í•˜ì—¬ ë§¤ë²ˆ ì •ì˜í•˜ëŠ” ì˜¤ë²„í—¤ë“œ ì œê±°
        - í…ì„œ ì—°ì‚° ìµœì í™”: torch ë§ˆìŠ¤í‚¹ í™œìš©
        - ì„±ëŠ¥ ë¡œê¹…: Phase2PerfTimerë¡œ ì¸¡ì • ê°€ëŠ¥ (PHASE2_PERF_LOG=1)
        """
        with Phase2PerfTimer("_raw_prompt_ids", batch_size=rollings.batch['input_ids'].shape[0]):

            # [Phase 2] long() ë³€í™˜
            input_ids = rollings.batch['input_ids'].long()
            attention_mask = rollings.batch['attention_mask']

            # [Phase 2] ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í…ì„œ ë§ˆìŠ¤í‚¹ í™œìš©
            # ê° ìƒ˜í”Œë³„ë¡œ ìœ íš¨í•œ í† í°ë§Œ ì¶”ì¶œ
            batch_size = input_ids.shape[0]
            raw_next_obs_ids = []

            for idx in range(batch_size):
                # ë§ˆìŠ¤í¬ê°€ 1ì¸ ìœ„ì¹˜ì˜ í† í°ë§Œ ì¶”ì¶œ
                valid_ids = input_ids[idx][attention_mask[idx] == 1].tolist()

                # ì—°ì†ëœ ì´ë¯¸ì§€ í† í° ì••ì¶• (ì¸ë¼ì¸ ì²˜ë¦¬)
                compressed = self._compress_consecutive_tokens(valid_ids, 151655)
                raw_next_obs_ids.append(compressed)

            raw_next_obs_ids = np.array(raw_next_obs_ids, dtype=object)
            rollings.non_tensor_batch['raw_prompt_ids'] = raw_next_obs_ids
            rollings.batch['input_ids'] = input_ids  # long() ë³€í™˜ëœ ë²„ì „ ì €ì¥

            return rollings

    def _compress_consecutive_tokens(self, arr: list, target: int) -> list:
        """
        [Phase 2] ì—°ì†ëœ target í† í°ì„ í•˜ë‚˜ë¡œ ì••ì¶•í•©ë‹ˆë‹¤.

        Args:
            arr: í† í° ID ë¦¬ìŠ¤íŠ¸
            target: ì••ì¶•í•  ëŒ€ìƒ í† í° (ì˜ˆ: 151655 = ì´ë¯¸ì§€ í† í°)

        Returns:
            ì••ì¶•ëœ í† í° ë¦¬ìŠ¤íŠ¸
        """
        if not arr:
            return arr

        result = []
        i = 0
        n = len(arr)

        while i < n:
            if arr[i] == target:
                result.append(target)
                # ì—°ì†ëœ target ê±´ë„ˆë›°ê¸°
                while i + 1 < n and arr[i + 1] == target:
                    i += 1
            else:
                result.append(arr[i])
            i += 1

        return result

    def deactivate_batch(self, active_mask,rollings):
        raw_prompt_ids = rollings.non_tensor_batch['raw_prompt_ids']
        max_model_len = 10240
        curr_active_mask = torch.tensor([len(raw_prompt_ids_item) < max_model_len for raw_prompt_ids_item in raw_prompt_ids], dtype=torch.bool)
        active_mask = active_mask * curr_active_mask
        return active_mask

    def run_llm_loop(self, gen_batch, initial_input_ids: torch.Tensor) -> Tuple[Dict, Dict]:
        """Run main LLM generation loop."""

        meta_info = {}

        # [FIX] gen_batchë¥¼ ë©¤ë²„ë¡œ ì €ì¥í•˜ì—¬ _collect_samples_dataì—ì„œ ground_truth ì ‘ê·¼ ê°€ëŠ¥
        self._current_gen_batch = gen_batch

        # [NEW] ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì´ˆê¸°í™”
        if self.streaming_reward_manager:
            self._init_prompt_tracking(gen_batch)

        # [Phase 6] ì™„ì „ ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ì´ˆê¸°í™”
        self._pending_threads.clear()
        self.generated_answers.clear()
        self._streaming_frozen_generated.clear()

        # â–¼â–¼â–¼[ì„±ëŠ¥ ì¸¡ì • ì¶”ê°€] 1. ë¡œê·¸ íŒŒì¼ ë° ëª¨ë‹ˆí„° ê°ì²´ ì´ˆê¸°í™”â–¼â–¼â–¼ ìˆ˜ì •
        # ê³ ìœ í•œ ë¡œê·¸ íŒŒì¼ ì´ë¦„ì„ ìƒì„±í•˜ì—¬ ëª¨ë“  ì¸¡ì • ê²°ê³¼ë¥¼ í•œ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤.
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"./logs/generation_detail_{current_time}_{uuid.uuid4().hex[:6]}.txt"
        
        # ì¸¡ì • ì§€ì  1: ë©”ì¸ ëª¨ë¸(Actor)ì˜ 'ê³„íš' ìƒì„± ì„±ëŠ¥ ì¸¡ì •ìš©
        actor_monitor = GPUMonitor(log_file=log_filename, label="[1] Actor Generation (Planning)")
        
        # ì¸¡ì • ì§€ì  2: ì™¸ë¶€ ë„êµ¬(ê²€ìƒ‰ API) í˜¸ì¶œ ì‹œê°„ ì¸¡ì •ìš©
        tool_monitor = GPUMonitor(log_file=log_filename, label="[2] Tool Execution (Search API)")
        
        # ì¸¡ì • ì§€ì  3: Frozen ëª¨ë¸ì˜ 'ìµœì¢… ë‹µë³€' ìƒì„± ì„±ëŠ¥ ì¸¡ì •ìš©
        frozen_monitor = GPUMonitor(log_file=log_filename, label="[3] Frozen Generator (Answering)")
        # â–²â–²â–²[ì„±ëŠ¥ ì¸¡ì • ì¶”ê°€]â–²â–²â–²        

        original_left_side = {'input_ids': initial_input_ids}
        original_right_side = {'responses': initial_input_ids[:, []]}

        
        active_mask = torch.ones(gen_batch.batch['input_ids'].shape[0], dtype=torch.bool)
        active_num_list = [active_mask.sum().item()]
        rollings = gen_batch
        raw_prompt_ids = rollings.non_tensor_batch['raw_prompt_ids']

        #generator added
        self.search_completed = [False] * gen_batch.batch['input_ids'].shape[0]

        # ===== (4) ì²« í„´ì—ì„œ ì§ˆë¬¸ ë¬¸ìì—´ ì €ì¥(ì›ë˜ íŒŒì‹± ë°©ì‹) & ì»¨í…Œì´ë„ˆ ì¤€ë¹„ =====
        decoded_inputs = self.tokenizer.batch_decode(initial_input_ids, skip_special_tokens=True)
        '''
        ìµœì¢… generatorì—ê²Œ ì´ˆë°˜ ì¿¼ë¦¬ë¥¼ ë„˜ê²¨ì£¼ê¸° ìœ„í•´ì„œ.
        '''
        self.questions = []
        for s in decoded_inputs:
            try:
                q = s.split('Question: ')[1].split(' \n\nassistant\n')[0]
            except Exception:
                q = s  # fallback
            self.questions.append(q)
        #


        self.retrievaled_images = [[] for _ in range(gen_batch.batch['input_ids'].shape[0])]
        self.cropped_images = [[] for _ in range(gen_batch.batch['input_ids'].shape[0])]      # generator added

        # [Phase 3] ë£¨í”„ ë¶ˆë³€ê°’ ì¶”ì¶œ: max_turns - 1 ê³„ì‚°ì„ ë£¨í”„ ì™¸ë¶€ë¡œ ì´ë™
        last_turn_idx = self.config.max_turns - 1

        ############======================ğŸš€Main generation loopğŸš€==================######################
        for step in range(self.config.max_turns):
            if not active_mask.sum():
                break
            # [Phase 3] ë£¨í”„ ë¶ˆë³€ ìƒìˆ˜ ì‚¬ìš©
            rollings.batch = self.tensor_fn.cut_to_effective_len(
                rollings.batch,
                keys=_CUT_KEYS
            ) #ë°ì´í„° ì••ì¶•

            rollings = self._raw_prompt_ids(rollings)#ì „ì²˜ë¦¬ 

            active_mask = self.deactivate_batch(active_mask, rollings) #ìµœëŒ€ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ deactivate
            if not active_mask.sum():
                break
            
            #ìˆ˜ì • ì œê±° max turn 5
            # if 'multi_modal_inputs' in rollings.non_tensor_batch.keys():
            #     rollings_active = DataProto.from_dict(
            #         tensors={k: v[active_mask] for k, v in rollings.batch.items()},
            #         non_tensors={k: v[active_mask] for k, v in rollings.non_tensor_batch.items()}
            #     )
            # else:
            #     rollings_active = DataProto.from_dict({
            #         k: v[active_mask] for k, v in rollings.batch.items()
            #     })  
            
            # [Phase 3] ë£¨í”„ ë¶ˆë³€ê°’ ì‚¬ìš©: last_turn_idx
            is_last_turn = step == last_turn_idx

            if not is_last_turn:
                # [Phase 3] .keys() ì œê±°: in dictê°€ ë” íš¨ìœ¨ì 
                if 'multi_modal_inputs' in rollings.non_tensor_batch:
                    rollings_active = DataProto.from_dict(
                        tensors={k: v[active_mask] for k, v in rollings.batch.items()},
                        non_tensors={k: v[active_mask] for k, v in rollings.non_tensor_batch.items()}
                    )
                else:
                    rollings_active = DataProto.from_dict({
                        k: v[active_mask] for k, v in rollings.batch.items()
                    })
            #


            #ìˆ˜ì • maxturn 5
            # actor_monitor.start() #ì¸¡ì • ì§€ì  1: 'ê³„íš' ìƒì„± ì„±ëŠ¥ ì¸¡ì • ìˆ˜ì •
            # gen_output = self._generate_with_gpu_padding(rollings_active)
            # actor_monitor.stop() #ì¸¡ì • ë
                actor_monitor.start() #ì¸¡ì • ì§€ì  1: 'ê³„íš' ìƒì„± ì„±ëŠ¥ ì¸¡ì • ìˆ˜ì •
                gen_output = self._generate_with_gpu_padding(rollings_active)
                actor_monitor.stop() #ì¸¡ì • ë            
            #//    

            #ìˆ˜ì • max turn 5
            #meta_info = gen_output.meta_info     
                meta_info = gen_output.meta_info
            #//

            #ìˆ˜ì • mac turn5
            # responses_ids, responses_str = self._postprocess_responses(gen_output.batch['responses'])
            # print(responses_str[0])
                responses_ids, responses_str = self._postprocess_responses(gen_output.batch['responses'])
                print(responses_str[0])
            else:
                forced_count = active_mask.sum().item()
                responses_str = [FORCED_COMPLETION_RESPONSE] * forced_count
                if forced_count > 0:
                    responses_ids = self._batch_tokenize(responses_str)
                else:
                    responses_ids = torch.empty((0, 0), dtype=rollings.batch['input_ids'].dtype)
            #//            


            
            # Execute in environment and process observations
            
            #ê°œë³„ ì˜ˆì œ(example) ìˆ˜ì¤€ì—ì„œ ë¹ˆìë¦¬ë¥¼ ì±„ì›Œì£¼ëŠ”(pad)'
            responses_ids, responses_str = self.tensor_fn._example_level_pad(responses_ids, responses_str, active_mask)

            #ìˆ˜ì • ì¶”ê°€ max turn 5
            responses_ids = responses_ids.to(rollings.batch['input_ids'].device)
            #//


            #ìˆ˜ì •----#
            # 1. execute_predictionsë¥¼ í˜¸ì¶œí•˜ê¸° ì „ì— uidsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤

            all_uids = rollings.non_tensor_batch['id']


            # 2. Execute in environment and process observations
            #    í˜¸ì¶œ ì‹œ uidsë¥¼ ë‘ ë²ˆì§¸ ì¸ìë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

            tool_monitor.start() #'í–‰ë™'ì„ ìœ„í•œ ì™¸ë¶€ ë„êµ¬ í˜¸ì¶œ ì‹œê°„ ì¸¡ì •â–¼â–¼â–¼ ìˆ˜ì •
            next_obs, dones = self.execute_predictions(responses_str, all_uids, self.tokenizer.pad_token, active_mask)
            tool_monitor.stop() #ì¸¡ì • ë

            # --- ì—¬ê¸°ê¹Œì§€ ---

            #next_obs, dones = self.execute_predictions(responses_str, self.tokenizer.pad_token, active_mask) #ìˆ˜ì • ì œê±° uid ë„˜ê¸°ê¸°
            
            curr_active_mask = torch.tensor([not done for done in dones], dtype=torch.bool)
            active_mask = active_mask * curr_active_mask
            active_num_list.append(active_mask.sum().item())
            next_obs_ids, next_obs_str, next_obs_multi_modal_data, next_obs_multi_modal_inputs = self._process_next_obs(next_obs, rollings)
            
            rollings = self._concat_multi_modal_data(
                rollings,
                next_obs_multi_modal_data,
                next_obs_multi_modal_inputs
            )
            
            # Update states            
            rollings = self._update_rolling_state(
                rollings,
                responses_ids, #ìˆ˜ì • ì œê±° 
                #padded_responses_ids, #ìˆ˜ì • ì¶”ê°€ uid
                next_obs_ids
            )
            original_right_side = self._update_right_side(
                original_right_side,
                responses_ids, #ìˆ˜ì • ì œê±° uid
                #padded_responses_ids, #ìˆ˜ì • ì¶”ê°€ uid
                next_obs_ids
            )



        # final LLM rollout
        if active_mask.sum():

            # [Phase 3] ë£¨í”„ ë¶ˆë³€ ìƒìˆ˜ ì‚¬ìš©
            rollings.batch = self.tensor_fn.cut_to_effective_len(
                rollings.batch,
                keys=_CUT_KEYS
            )

            rollings = self._raw_prompt_ids(rollings)

            active_mask = self.deactivate_batch(active_mask, rollings)

            if active_mask.sum():
                # [Phase 3] .keys() ì œê±°: in dictê°€ ë” íš¨ìœ¨ì 
                if 'multi_modal_inputs' in rollings.non_tensor_batch:
                    rollings_active = DataProto.from_dict(
                        tensors={k: v[active_mask] for k, v in rollings.batch.items()},
                        non_tensors={k: v[active_mask] for k, v in rollings.non_tensor_batch.items()}
                    )
                else:
                    rollings_active = DataProto.from_dict({
                        k: v[active_mask] for k, v in rollings.batch.items()
                    })

                gen_output = self._generate_with_gpu_padding(rollings_active)

                meta_info = gen_output.meta_info
                responses_ids, responses_str = self._postprocess_responses(gen_output.batch['responses'])
                responses_ids, responses_str = self.tensor_fn._example_level_pad(responses_ids, responses_str, active_mask)

                all_uids = rollings.non_tensor_batch['id'] #ìˆ˜ì • uid ì¶”ê°€ 


                # # Execute in environment and process observations
                _, dones = self.execute_predictions( #ctive uid ì¶”ê°€ ìˆ˜ì •
                    responses_str, all_uids, self.tokenizer.pad_token, active_mask, do_search=False
                )

                curr_active_mask = torch.tensor([not done for done in dones], dtype=torch.bool)
                active_mask = active_mask * curr_active_mask
                active_num_list.append(active_mask.sum().item())

                original_right_side = self._update_right_side(
                    original_right_side,
                    responses_ids,
                )
        
        print("ACTIVE_TRAJ_NUM:", active_num_list)
        
        # =================== raw prompt ids ===================
        rollings.non_tensor_batch['raw_prompt_ids'] = raw_prompt_ids
        # rollings.non_tensor_batch.pop('raw_prompt_ids')
        
        if not self.is_validation:
            rollings, original_right_side = self._add_noisy_multi_modal_data(rollings, original_right_side)
        ### check again
        
        retrievaled_images_array = np.empty(len(self.retrievaled_images), dtype=object)
        for idx in range(len(self.retrievaled_images)):
            retrievaled_images_array[idx] = self.retrievaled_images[idx]
        rollings.non_tensor_batch['retrievaled_images'] = retrievaled_images_array
        # ===== generator added=====
        gen_to_tokenize = [""] * len(self.retrievaled_images)

        # =========================================================================
        # [Phase 6] ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        # - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œ ë°±ê·¸ë¼ìš´ë“œë¡œ ì²˜ë¦¬ ì¤‘ì¸ Frozen Generator ì™„ë£Œ ëŒ€ê¸°
        # =========================================================================
        if self._pending_threads:
            pending_count = len(self._pending_threads)
            print(f"[Phase 6] ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ëŒ€ê¸° ì¤‘... ({pending_count}ê°œ)")
            wait_start = _time.perf_counter()

            for thread in self._pending_threads:
                thread.join(timeout=120)  # ìµœëŒ€ 2ë¶„ ëŒ€ê¸°

            wait_elapsed = _time.perf_counter() - wait_start
            print(f"[Phase 6] ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì™„ë£Œ: {pending_count}ê°œ, {wait_elapsed:.2f}ì´ˆ")
            self._pending_threads.clear()

        # [Phase 6] ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ ìƒ˜í”Œ ê±´ë„ˆë›°ê¸°
        # - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œ ì´ë¯¸ Frozen Generatorê°€ í˜¸ì¶œëœ ìƒ˜í”Œì€ ì œì™¸
        completed_indices = [
            i for i, flag in enumerate(self.search_completed)
            if flag and i not in self._streaming_frozen_generated
        ]

        if completed_indices:
            batch_questions = []
            batch_paths = []

            for i in completed_indices:
                q = self.questions[i]
                paths = self._prepare_generator_images(self.retrievaled_images[i], self.cropped_images[i])
                batch_questions.append(q)
                batch_paths.append(paths)

            frozen_monitor.start()
            index2answer = self._call_frozen_generator_batch(
                completed_indices, batch_questions, batch_paths
            )
            frozen_monitor.stop()

            # ìƒˆë¡œ ìƒì„±ëœ ë‹µë³€ ì €ì¥
            for i in completed_indices:
                ans = index2answer.get(i, "")
                self.generated_answers[i] = ans

        # [Phase 6] ëª¨ë“  ì™„ë£Œëœ ìƒ˜í”Œì˜ ë‹µë³€ ì ìš© (ìŠ¤íŠ¸ë¦¬ë° + ë°°ì¹˜ ëª¨ë‘ í¬í•¨)
        for i, flag in enumerate(self.search_completed):
            if flag:
                ans = self.generated_answers.get(i, "")
                if ans:
                    gen_to_tokenize[i] = f"<answer>{ans}</answer>{self.tokenizer.eos_token}"

        ans_ids = self.tokenizer(
            gen_to_tokenize, padding='longest', return_tensors='pt', add_special_tokens=False
        )['input_ids']

        original_right_side = self._update_right_side(original_right_side, ans_ids)
        rollings = self._update_rolling_state(
            rollings, ans_ids, next_obs_ids=torch.zeros((ans_ids.shape[0], 0), dtype=torch.long)
        )

        # [Phase 4] ëª¨ë“  ë¹„ë™ê¸° ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ ëŒ€ê¸°
        saved_count = self._ensure_saves_complete()
        if saved_count > 0:
            print(f"[Phase 4] Async image saves completed: {saved_count}")

        return self._compose_final_output(original_left_side, original_right_side, meta_info, rollings)
    
    def _add_noisy_multi_modal_data(self, rollings, original_right_side):
        image_padded = Image.new('RGB', (64, 64), (0, 0, 0))

        image_padded = process_image(image_padded, 256*256, 128*128)
        image_inputs = self.processor.image_processor([image_padded], return_tensors='pt')
        image_grid_thw = image_inputs['image_grid_thw']
        merge_length = self.processor.image_processor.merge_size**2
        padded_str = f"\n<|im_start|>user\n<|vision_start|>{self.processor.image_token * (image_grid_thw.prod() // merge_length)}<|vision_end|><|im_end|>"

        padded_str_list = []
        for idx, multi_modal_item in enumerate(rollings.non_tensor_batch['multi_modal_data']):
            if len(multi_modal_item['image']) == 0:
                padded_str_list.append(padded_str)
                rollings.non_tensor_batch['multi_modal_data'][idx]['image'].append(image_padded)
                rollings.non_tensor_batch['multi_modal_inputs'][idx] = image_inputs
            else:
                padded_str_list.append('')
            
        padded_ids = self.tokenizer(
            padded_str_list, 
            padding='longest',
            return_tensors='pt',
            add_special_tokens=False,  # Prevents adding special tokens
        )['input_ids']

        original_right_side = self._update_right_side(
            original_right_side,
            padded_ids
        )
        return rollings, original_right_side


    def _compose_final_output(self, left_side: Dict,
                            right_side: Dict,
                            meta_info: Dict,
                            rollings) -> Tuple[Dict, Dict]:
        """Compose final generation output."""
        final_output = right_side.copy()
        final_output['prompts'] = left_side['input_ids']
        
        # Combine input IDs
        final_output['input_ids'] = torch.cat([
            left_side['input_ids'],
            right_side['responses']
        ], dim=1)
        
        # Create attention mask and position ids
        final_output['attention_mask'] = torch.cat([
            self.tensor_fn.create_attention_mask(left_side['input_ids']),
            self.tensor_fn.create_attention_mask(final_output['responses'])
        ], dim=1)
        
        final_output['position_ids'] = self.tensor_fn.create_position_ids(
            final_output['attention_mask']
        )

        final_output = DataProto.from_dict(final_output,rollings.non_tensor_batch)
        final_output.meta_info.update(meta_info)
        
        return final_output

    # =========================================================================
    # [Phase 8] Async Pipeline & Aiohttp Search
    # =========================================================================
    def _get_aio_session(self):
        """Lazy initialization of aiohttp session."""
        if self._aio_search_session is None or self._aio_search_session.closed:
            # Check for running loop
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                # No loop running, session creation deferred or created without loop
                loop = None
            
            timeout = aiohttp.ClientTimeout(total=self.config.search_timeout)
            connector = aiohttp.TCPConnector(limit=self.config.search_max_workers * 2) 
            # Note: ClientSession typically needs to be created inside a coroutine
            # but if we are here, we might be inside one.
            self._aio_search_session = aiohttp.ClientSession(connector=connector, timeout=timeout)
        return self._aio_search_session

    async def _async_search_batches_aio(self, search_requests: List[Dict]) -> Dict[int, List]:
        """
        [Phase 8] True Async Search using aiohttp.
        Replaces ThreadPoolExecutor + requests with asyncio + aiohttp.
        """
        if not search_requests:
            return {}

        batch_size = self.config.search_batch_size
        # Split into batches
        batches = [
            search_requests[i:i + batch_size]
            for i in range(0, len(search_requests), batch_size)
        ]

        url = self.config.search_url
        max_retries = 3
        
        start_time = _time.perf_counter()
        _phase2_logger.info(f"[Tool:Search] Starting async search for {len(search_requests)} requests ({len(batches)} batches).")

        # [Fix] Create session per pipeline execution to avoid 'Event loop is closed'
        timeout = aiohttp.ClientTimeout(total=self.config.search_timeout)
        connector = aiohttp.TCPConnector(limit=self.config.search_max_workers * 2)

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            async def _fetch_batch(batch, batch_idx):
                batch_start = _time.perf_counter()
                for attempt in range(max_retries):
                    try:
                        async with session.post(url, json=batch) as response:
                            response.raise_for_status()
                            data = await response.json()
                            duration = _time.perf_counter() - batch_start
                            _phase2_logger.debug(f"[Tool:Search] Batch {batch_idx} completed in {duration:.3f}s ({len(data)} results)")
                            return data
                    except Exception as e:
                        if attempt == max_retries - 1:
                            _phase2_logger.error(f"[Tool:Search] Batch {batch_idx} failed after {max_retries} retries: {e}")
                            return [] # Return empty on failure
                        
                        wait_time = 0.5 * (2 ** attempt)
                        _phase2_logger.warning(f"[Tool:Search] Batch {batch_idx} retry {attempt+1}/{max_retries} (error: {e}). Waiting {wait_time:.1f}s.")
                        await asyncio.sleep(wait_time) # Exponential backoff
                return []

            # Execute all batches concurrently
            results_list = await asyncio.gather(*[_fetch_batch(b, i) for i, b in enumerate(batches)])
        
        # Flatten results
        all_results = []
        for res in results_list:
            if res:
                all_results.extend(res)

        elapsed = _time.perf_counter() - start_time
        _phase2_logger.info(f"[Tool:Search] All batches completed in {elapsed:.3f}s. Total results received: {len(all_results)}")

        # Map to request_idx
        results_map = {
            item['request_idx']: item.get('results', [])
            for item in all_results
        }
        return results_map

    async def _execute_async_pipeline(self, predictions: List[str], uids: np.ndarray, pad_token: str, active_mask=None, do_search=True) -> Tuple[List[Any], List[Any]]:
        """
        [Phase 8] ì™„ì „ ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ (Async Pipeline)
        Search (I/O)ì™€ BBox Crop/Process (CPU)ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        pipeline_start = _time.perf_counter()
        
        cur_actions, contents = self.postprocess_predictions(predictions)
        n_samples = len(cur_actions)
        next_obs = [None] * n_samples
        dones = [None] * n_samples
        
        # 1. Search Request ìˆ˜ì§‘
        search_requests = []
        search_indices = []
        for i, (action, content) in enumerate(zip(cur_actions, contents)):
            if action == 'search':
                m = _RE_UID_SUFFIX.search(str(uids[i]))
                search_id = int(m.group(1)) if m else -1
                search_requests.append({
                    "query": content,
                    "id": str(search_id),
                    "request_idx": i
                })
                search_indices.append(i)

        # 2. Search Task ì‹œì‘ (Non-blocking)
        search_task = None
        if do_search and search_requests:
            _phase2_logger.info(f"[Pipeline] Launching async search task for {len(search_requests)} requests.")
            # aiohttp ê¸°ë°˜ ë¹„ë™ê¸° ê²€ìƒ‰ ì‹œì‘
            search_task = asyncio.create_task(self._async_search_batches_aio(search_requests))

        # 3. BBox ë° ê¸°íƒ€ ì•¡ì…˜ ì²˜ë¦¬ (CPU ì‘ì—… - Search ëŒ€ê¸° ì‹œê°„ ë™ì•ˆ ìˆ˜í–‰)
        bbox_queue = deque(content for action, content in zip(cur_actions, contents) if action == 'bbox')
        
        for i, (action, active) in enumerate(zip(cur_actions, active_mask)):
            if not active:
                next_obs[i] = ''
                dones[i] = 1
                continue
            
            if action == 'search':
                dones[i] = 0
                # next_obs[i]ëŠ” ë‚˜ì¤‘ì— search_task ì™„ë£Œ í›„ ì±„ì›€
            
            elif action == 'bbox':
                dones[i] = 0
                try:
                    bbox_value = json.loads(bbox_queue.popleft())
                    # ìœ íš¨ì„± ê²€ì‚¬
                    if len(bbox_value) == 4 and all(v >= 0 for v in bbox_value):
                        next_obs[i] = bbox_value
                    else:
                        raise ValueError("Invalid bbox value (length!=4 or negative value)")
                except Exception as e:
                    reason = str(e)
                    next_obs[i] = _format_invalid_bbox_message(reason)
            
            elif action == 'search_complete':
                is_true = contents[i].strip().lower() == 'true'
                if is_true:
                    self.search_completed[i] = True
                    if self.streaming_reward_manager:
                        self._check_and_submit_prompt_reward(i)
                next_obs[i] = ''
                dones[i] = 1
            
            else:
                next_obs[i] = _MSG_INVALID_ACTION
                dones[i] = 0

        # 4. Search ê²°ê³¼ ëŒ€ê¸° ë° ë³‘í•©
        if search_task:
            try:
                # CPU ì‘ì—…ì´ ëë‚œ í›„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¼ (Overlapping íš¨ê³¼)
                results_map = await search_task
                
                pipeline_duration = _time.perf_counter() - pipeline_start
                _phase2_logger.info(f"[Pipeline] Search results received. Total pipeline latency: {pipeline_duration:.3f}s")
                
                for i in search_indices:
                    if active_mask[i]:
                        next_obs[i] = results_map.get(i, [])
            except Exception as e:
                _phase2_logger.error(f"[AsyncPipeline] Search failed: {e}")
                # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                for i in search_indices:
                    if active_mask[i]:
                        next_obs[i] = []

        # 5. ì•ˆì „ ì¥ì¹˜ (None ì œê±°)
        for i in range(n_samples):
            if next_obs[i] is None:
                next_obs[i] = []
            if dones[i] is None:
                dones[i] = 0
                
        return next_obs, dones

    # =========================================================================
    # [NEW] ë¹„ë™ê¸° ë³‘ë ¬ ê²€ìƒ‰ ë©”ì„œë“œ
    # =========================================================================
    def _search_single_batch(self, batch_reqs: List[Dict], max_retries: int = 3) -> List[Dict]:
        """
        ë‹¨ì¼ ë°°ì¹˜ ê²€ìƒ‰ ìš”ì²­ (ì›Œì»¤ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
        ì‹¤íŒ¨ ì‹œ ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„

        ìµœì í™”:
        - HTTP ì—°ê²° í’€ë§ ì‚¬ìš© (self._search_session)
        - configì—ì„œ timeout ì„¤ì • (ê¸°ë³¸ 60ì´ˆ)
        """
        last_error = None
        timeout = self.config.search_timeout  # ê¸°ë³¸ê°’ 60ì´ˆ

        for attempt in range(max_retries):
            try:
                # [ìµœì í™”] Session ì‚¬ìš©ìœ¼ë¡œ ì—°ê²° ì¬ì‚¬ìš©
                response = self._search_session.post(
                    self.config.search_url,
                    json=batch_reqs,
                    timeout=timeout
                )
                response.raise_for_status()
                return response.json()
            except Exception as e:
                last_error = e
                wait_time = (2 ** attempt) + _random.uniform(0, 1)
                print(f"[Search] ë°°ì¹˜ ê²€ìƒ‰ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    _time.sleep(wait_time)

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        raise RuntimeError(f"ê²€ìƒ‰ ë°°ì¹˜ {max_retries}íšŒ ì¬ì‹œë„ ì‹¤íŒ¨: {last_error}")

    def _async_search_batches(self, search_requests: List[Dict]) -> Dict[int, List]:
        """
        ë¹„ë™ê¸° ë³‘ë ¬ ê²€ìƒ‰ - ThreadPoolExecutorë¡œ ë°°ì¹˜ë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬

        Args:
            search_requests: ê²€ìƒ‰ ìš”ì²­ ë¦¬ìŠ¤íŠ¸ [{query, id, request_idx}, ...]

        Returns:
            request_idx -> results ë§¤í•‘

        Raises:
            RuntimeError: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ (ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨)
        """
        if not search_requests:
            return {}

        batch_size = self.config.search_batch_size
        max_workers = self.config.search_max_workers

        # ë°°ì¹˜ë¡œ ë¶„í• 
        batches = [
            search_requests[i:i + batch_size]
            for i in range(0, len(search_requests), batch_size)
        ]

        all_results = []
        errors = []

        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self._search_single_batch, batch): idx
                for idx, batch in enumerate(batches)
            }

            for future in as_completed(futures):
                batch_idx = futures[future]
                try:
                    batch_results = future.result()
                    all_results.extend(batch_results)
                except Exception as e:
                    errors.append((batch_idx, str(e)))

        # ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
        if errors:
            error_msg = "; ".join([f"ë°°ì¹˜{idx}: {err}" for idx, err in errors])
            raise RuntimeError(f"ë³‘ë ¬ ê²€ìƒ‰ ì‹¤íŒ¨: {error_msg}")

        # ê²°ê³¼ ë§¤í•‘ ìƒì„±
        results_map = {
            item['request_idx']: item.get('results', [])
            for item in all_results
        }

        return results_map

    # execute_predictions í•¨ìˆ˜
    def execute_predictions(self, predictions: List[str], uids: np.ndarray, pad_token: str, active_mask=None, do_search=True) -> List[str]:
        """
        [Phase 8] Async Pipeline Wrapper
        
        ê¸°ì¡´ ë™ê¸° ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë‚´ë¶€ì ìœ¼ë¡œ _execute_async_pipelineì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
        """
        try:
            # 1. ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ í™•ì¸
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            if loop is not None:
                # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ” ê²½ìš° (ì˜ˆ: Ray Worker, Jupyter)
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(
                        asyncio.run, 
                        self._execute_async_pipeline(predictions, uids, pad_token, active_mask, do_search)
                    )
                    return future.result(timeout=120) # 2ë¶„ íƒ€ì„ì•„ì›ƒ
            else:
                # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš°
                return asyncio.run(
                    self._execute_async_pipeline(predictions, uids, pad_token, active_mask, do_search)
                )
        except Exception as e:
            print(f"[ExecutePredictions] Async pipeline failed, error: {e}")
            import traceback
            traceback.print_exc()
            # ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            n_samples = len(predictions)
            return [[]] * n_samples, [0] * n_samples

    def postprocess_predictions(self, predictions: List[Any]) -> Tuple[List[int], List[bool]]:
        """
        Process (text-based) predictions from llm into actions and validity flags.
        
        Args:
            predictions: List of raw predictions
            
        Returns:
            Tuple of (actions list, validity flags list)
        """
        actions = []
        contents = []
                
        for prediction in predictions:
            if isinstance(prediction, str): # for llm output

                #ìˆ˜ì • max turn 5
                # pattern = r'<(search|bbox|search_complete)>(.*?)</\1>'
                # match = re.search(pattern, prediction, re.DOTALL)
                # if match:
                #     content = match.group(2).strip()  # Return only the content inside the tags
                #     action = match.group(1)
                stripped_prediction = prediction.strip()
                if stripped_prediction == FORCED_COMPLETION_RESPONSE:
                    content = 'true'
                    action = 'search_complete'
                #//

                else:
                    # [Phase 1] ì‚¬ì „ ì»´íŒŒì¼ëœ ì •ê·œì‹ ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
                    match = _RE_ACTION_PATTERN.search(prediction)
                    if match:
                        content = match.group(2).strip()  # Return only the content inside the tags
                        action = match.group(1)
                    else:
                        content = ''
                        action = None                    
            else:
                raise ValueError(f"Invalid prediction type: {type(prediction)}")
            
            actions.append(action)
            contents.append(content)
            
        return actions, contents

    #generator added
    # ===== (8) generator ì´ë¯¸ì§€ ì¤€ë¹„ =====
    def _prepare_generator_images(self, originals: List[str], crops: List[str]) -> List[str]:
        # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ, ì¤‘ë³µ ì œê±°, ìµœëŒ€ ì¥ìˆ˜ ì œí•œ
        seen = set()
        out = []
        for p in (originals + crops):
            if p and (p not in seen) and os.path.exists(p):
                seen.add(p)
                out.append(p)
            if len(out) >= self.config.generator_max_images:
                break
        return out



    def _call_frozen_generator_single(self, question: str, image_paths: List[str]) -> Tuple[int, str]:
        if not _HAS_DASHSCOPE:
            print("ğŸš¨ ì˜¤ë¥˜: DashScope ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. (.env í‚¤ í™•ì¸ ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”)") # ë””ë²„ê¹…
            return (0, "")

        try:
            # ë¹ˆ í”„ë¡¬í”„íŠ¸ ë°©ì§€(400 íšŒí”¼)
            qtext = (question or "").strip() or "."

            sys_prompt = (
                "You are a visual QA generator. "
                "Use only the provided images and the user question. "
                "Return ONLY the final answer text without extra explanations."
            )

            # ì´ë¯¸ì§€ íŒŒíŠ¸ êµ¬ì„± (file:// ê°•ì œ)
            user_content = []
            if image_paths:
                for p in image_paths:
                    part = _to_image_part(p)  # >>> ADDED: helper ì‚¬ìš©
                    if part:
                        user_content.append(part)
            user_content.append({"text": f"Question: {qtext}"})

            messages = []
            if getattr(self.config, "use_system_prompt", True):
                messages.append({"role": "system", "content": [{"text": sys_prompt}]})
            messages.append({"role": "user", "content": user_content})

            try:
                resp = _dashscope_call_with_fallback(
                    model=self.config.frozen_model,
                    messages=messages,
                    max_tokens=int(getattr(self.config, "frozen_max_tokens", 256)),
                )
            except Exception:
                print(f"ğŸš¨ [API ERROR] Question: {question[:30]}... | Error: {e}")  # ë””ë²„ê¹…
                return (0, "")

            code = getattr(resp, "status_code", None)
            if code == HTTPStatus.OK:
                text = _extract_text_from_multimodal(resp) or ""
                return (200, text)
            
            return (int(code) if isinstance(code, HTTPStatus) else (code or 0), "")
        except Exception:
            print(f"ğŸš¨ ì˜¤ë¥˜: API í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}") # ë””ë²„ê¹…
            return (0, "")


    def _call_frozen_generator_batch(
        self,
        indices: List[int],
        questions: List[str],
        images_list: List[List[str]],
    ) -> Dict[int, str]:
        """
        Frozen Generator ë°°ì¹˜ í˜¸ì¶œ (ë™ê¸° ì¸í„°í˜ì´ìŠ¤)

        ë‚´ë¶€ì ìœ¼ë¡œ OpenAI AsyncClientê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ë¹„ë™ê¸° ì²˜ë¦¬,
        ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ DashScope SDK ë™ê¸° ë°©ì‹ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.
        """
        results: Dict[int, str] = {}
        if not indices:
            return results

        # [Phase 5] OpenAI AsyncClient ì‚¬ìš© ê°€ëŠ¥ì‹œ ë¹„ë™ê¸° ì²˜ë¦¬
        if _HAS_OPENAI_ASYNC and _OPENAI_ASYNC_CLIENT is not None:
            return self._call_frozen_generator_batch_async_wrapper(
                indices, questions, images_list
            )

        # === í´ë°±: ê¸°ì¡´ DashScope SDK ë™ê¸° ë°©ì‹ ===
        return self._call_frozen_generator_batch_sync(
            indices, questions, images_list
        )

    def _call_frozen_generator_batch_sync(
        self,
        indices: List[int],
        questions: List[str],
        images_list: List[List[str]],
    ) -> Dict[int, str]:
        """ê¸°ì¡´ DashScope SDK ë™ê¸° ë°©ì‹ (í´ë°±ìš©)"""
        results: Dict[int, str] = {}
        if not indices:
            return results

        workers = max(1, int(getattr(self.config, "generator_batch_workers", 4)))
        workers = min(workers, 4)
        max_retries = int(getattr(self.config, "frozen_max_retries", 3))
        backoff_base = float(getattr(self.config, "frozen_backoff_base", 1.5))

        def _once_with_retry(idx: int, q: str, paths: List[str]) -> Tuple[int, str]:
            delay = 0.0
            for attempt in range(max_retries):
                if delay > 0:
                    _time.sleep(delay)
                code, ans = self._call_frozen_generator_single(q, paths)

                if code == 200:
                    # [NEW] Frozen Generator ìƒì„¸ ë¡œê¹… (ì„±ê³µ - ë™ê¸°)
                    _log_frozen_generator_detail(
                        idx=idx,
                        question=q,
                        image_paths=paths,
                        answer=ans if ans else "",
                        status_code=code,
                        error=None
                    )
                    if ans:
                        return idx, ans
                    else:
                        return idx, ""

                if code in (429, 500, 502, 503, 504, 0):
                    delay = (backoff_base ** attempt) + _random.uniform(0, 0.2)
                    continue

                # [NEW] Frozen Generator ìƒì„¸ ë¡œê¹… (ì˜¤ë¥˜ - ë™ê¸°)
                _log_frozen_generator_detail(
                    idx=idx,
                    question=q,
                    image_paths=paths,
                    answer="",
                    status_code=code,
                    error=f"Non-retryable error code: {code}"
                )
                return idx, ""

            # [NEW] Frozen Generator ìƒì„¸ ë¡œê¹… (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ - ë™ê¸°)
            _log_frozen_generator_detail(
                idx=idx,
                question=q,
                image_paths=paths,
                answer="",
                status_code=0,
                error=f"Max retries ({max_retries}) exceeded (sync fallback)"
            )
            return idx, ""

        for start in range(0, len(indices), workers):
            end = start + workers
            chunk_idx = indices[start:end]
            chunk_q = questions[start:end]
            chunk_img = images_list[start:end]

            with ThreadPoolExecutor(max_workers=workers) as ex:
                futs = [ex.submit(_once_with_retry, i, q, p) for i, q, p in zip(chunk_idx, chunk_q, chunk_img)]
                for f in as_completed(futs):
                    try:
                        i, ans = f.result()
                    except Exception:
                        i, ans = None, ""
                    if i is not None:
                        results[i] = ans or ""

            _time.sleep(0.05)

        return results

    def _call_frozen_generator_batch_async_wrapper(
        self,
        indices: List[int],
        questions: List[str],
        images_list: List[List[str]],
    ) -> Dict[int, str]:
        """
        [Phase 5] ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ë™ê¸° ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘

        asyncio.run() ë˜ëŠ” ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        try:
            # ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            if loop is not None:
                # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ ì•ˆì— ìˆëŠ” ê²½ìš° (Ray ì›Œì»¤ ë“±)
                # nest_asyncio ë˜ëŠ” ThreadPoolExecutor ì‚¬ìš©
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(
                        asyncio.run,
                        self._call_frozen_generator_batch_async(
                            indices, questions, images_list
                        )
                    )
                    return future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            else:
                # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì‹¤í–‰
                return asyncio.run(
                    self._call_frozen_generator_batch_async(
                        indices, questions, images_list
                    )
                )
        except Exception as e:
            _phase2_logger.warning(f"[Phase5] Async batch failed, falling back to sync: {e}")
            return self._call_frozen_generator_batch_sync(indices, questions, images_list)

    async def _call_frozen_generator_batch_async(
        self,
        indices: List[int],
        questions: List[str],
        images_list: List[List[str]],
    ) -> Dict[int, str]:
        """
        [Phase 5] OpenAI AsyncClientë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬

        - asyncio.gather()ë¡œ ì „ì²´ ë°°ì¹˜ë¥¼ í•œ ë²ˆì— ë³‘ë ¬ ì²˜ë¦¬
        - ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ (rate limit ëŒ€ì‘)
        - ì¬ì‹œë„ ë¡œì§ í¬í•¨
        """
        results: Dict[int, str] = {}
        if not indices:
            return results

        # ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
        max_concurrent = int(getattr(self.config, "frozen_max_concurrent", 50))
        max_retries = int(getattr(self.config, "frozen_max_retries", 3))
        backoff_base = float(getattr(self.config, "frozen_backoff_base", 1.5))
        max_tokens = int(getattr(self.config, "frozen_max_tokens", 1024))
        model = getattr(self.config, "frozen_model", "qwen2.5-vl-72b-instruct")

        # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œìš© ì„¸ë§ˆí¬ì–´
        semaphore = asyncio.Semaphore(max_concurrent)

        start_time = _time.perf_counter()
        _phase2_logger.info(
            f"[Phase5] Starting async batch: {len(indices)} samples, "
            f"max_concurrent={max_concurrent}"
        )

        async def _single_with_retry(idx: int, q: str, paths: List[str]) -> Tuple[int, str]:
            """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë‹¨ì¼ ë¹„ë™ê¸° í˜¸ì¶œ"""
            delay = 0.0
            for attempt in range(max_retries):
                if delay > 0:
                    await asyncio.sleep(delay)

                if self.verbose_frozen >= 1:
                    q_snippet = (q[:160] + ("..." if len(q) > 160 else ""))
                    print(f"[FROZEN][REQUEST idx={idx} attempt={attempt+1}] q={q_snippet} | images={len(paths)}")
                    if self.verbose_frozen >= 2:
                        print(f"[FROZEN][REQUEST FULL idx={idx}] question={q}\n  images={paths}")
                code, ans = await _call_frozen_generator_async_single(
                    client=_OPENAI_ASYNC_CLIENT,
                    model=model,
                    question=q,
                    image_paths=paths,
                    max_tokens=max_tokens,
                    semaphore=semaphore,
                )

                if code == 200:
                    # [NEW] Frozen Generator ìƒì„¸ ë¡œê¹… (ì„±ê³µ)
                    _log_frozen_generator_detail(
                        idx=idx,
                        question=q,
                        image_paths=paths,
                        answer=ans if ans else "",
                        status_code=code,
                        error=None
                    )
                    if self.verbose_frozen >= 1:
                        ans_snippet = (ans[:160] + ("..." if len(ans) > 160 else "")) if ans else ""
                        print(f"[FROZEN][RESPONSE idx={idx}] status=200 answer={ans_snippet}")
                        if self.verbose_frozen >= 2:
                            print(f"[FROZEN][RESPONSE FULL idx={idx}] answer={ans}")
                    return (idx, ans if ans else "")

                # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì˜¤ë¥˜
                if code in (429, 500, 502, 503, 504, 408, 0):
                    delay = (backoff_base ** attempt) + _random.uniform(0, 0.3)
                    continue

                # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ë¹ˆ ê²°ê³¼ ë°˜í™˜
                # [NEW] Frozen Generator ìƒì„¸ ë¡œê¹… (ì˜¤ë¥˜)
                _log_frozen_generator_detail(
                    idx=idx,
                    question=q,
                    image_paths=paths,
                    answer="",
                    status_code=code,
                    error=f"Non-retryable error code: {code}"
                )
                return (idx, "")

            # [NEW] Frozen Generator ìƒì„¸ ë¡œê¹… (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼)
            _log_frozen_generator_detail(
                idx=idx,
                question=q,
                image_paths=paths,
                answer="",
                status_code=0,
                error=f"Max retries ({max_retries}) exceeded"
            )
            # ê¸°ë³¸ ë¬¸ìì—´ì„ ë°˜í™˜í•´ downstream EMPTY ì²˜ë¦¬ë¥¼ í”¼í•œë‹¤
            return (idx, "No answer")

        # ëª¨ë“  ìš”ì²­ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        tasks = [
            _single_with_retry(idx, q, paths)
            for idx, q, paths in zip(indices, questions, images_list)
        ]

        # asyncio.gatherë¡œ ì „ì²´ ë°°ì¹˜ ë™ì‹œ ì²˜ë¦¬
        task_results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ìˆ˜ì§‘
        success_count = 0
        for result in task_results:
            if isinstance(result, Exception):
                _phase2_logger.warning(f"[Phase5] Task exception: {result}")
                continue
            if isinstance(result, tuple) and len(result) == 2:
                idx, ans = result
                results[idx] = ans or ""
                if ans:
                    success_count += 1

        elapsed = _time.perf_counter() - start_time
        _phase2_logger.info(
            f"[Phase5] Async batch completed: {success_count}/{len(indices)} success, "
            f"elapsed={elapsed:.2f}s, "
            f"throughput={len(indices)/elapsed:.1f} req/s"
        )

        return results

    # =========================================================================
    # ìŠ¤íŠ¸ë¦¬ë° Reward ê´€ë ¨ ë©”ì„œë“œë“¤
    # =========================================================================
    def _init_prompt_tracking(self, gen_batch):
        """
        í”„ë¡¬í”„íŠ¸ë³„ ì™„ë£Œ ì¶”ì  ì´ˆê¸°í™”

        n_agent êµ¬ì¡°ì—ì„œ ê° í”„ë¡¬í”„íŠ¸ì˜ ìƒ˜í”Œë“¤ì„ ê·¸ë£¹í™”í•˜ì—¬ ì¶”ì í•©ë‹ˆë‹¤.
        í”„ë¡¬í”„íŠ¸ì˜ ëª¨ë“  ìƒ˜í”Œì´ ì™„ë£Œë˜ë©´ Reward ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤.
        """
        uids = gen_batch.non_tensor_batch.get('uid', gen_batch.non_tensor_batch.get('id', []))
        n_agent = getattr(self.config, 'n_agent', 8)  # ê¸°ë³¸ê°’ 8

        batch_size = len(uids)
        num_prompts = batch_size // n_agent

        self._prompt_completion_status.clear()

        for prompt_idx in range(num_prompts):
            base_idx = prompt_idx * n_agent
            # í”„ë¡¬í”„íŠ¸ IDëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì¶©ëŒì„ ë°©ì§€í•œë‹¤ (uuid ì „ì²´ ì‚¬ìš©).
            prompt_id = str(uids[base_idx])
            # í˜¹ì‹œ ë™ì¼ IDê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ í”„ë¡¬í”„íŠ¸ ì¸ë±ìŠ¤ë¥¼ ë§ë¶™ì—¬ ê³ ìœ ì„± ë³´ì¥
            if prompt_id in self._prompt_completion_status:
                prompt_id = f"{prompt_id}-{prompt_idx}"

            self._prompt_completion_status[prompt_id] = {
                'total_samples': n_agent,
                'completed_samples': 0,
                'sample_indices': list(range(base_idx, base_idx + n_agent)),
                'submitted': False
            }

        print(f"[Generation] ìŠ¤íŠ¸ë¦¬ë° ì¶”ì  ì´ˆê¸°í™”: {num_prompts}ê°œ í”„ë¡¬í”„íŠ¸, "
              f"ê° {n_agent}ê°œ ìƒ˜í”Œ")

    def _check_and_submit_prompt_reward(self, sample_idx: int):
        """
        [Phase 6] ìƒ˜í”Œ ì™„ë£Œ ì‹œ í”„ë¡¬í”„íŠ¸ ì „ì²´ ì™„ë£Œ ì—¬ë¶€ í™•ì¸ í›„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬

        ì™„ì „ ë¹„ë™ê¸° ë°©ì‹:
        1. í”„ë¡¬í”„íŠ¸ì˜ ëª¨ë“  ìƒ˜í”Œì´ ì™„ë£Œë˜ë©´ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        2. ë°±ê·¸ë¼ìš´ë“œì—ì„œ Frozen Generator í˜¸ì¶œ â†’ Reward ì œì¶œ
        3. ë©”ì¸ ë£¨í”„ëŠ” ë¸”ë¡œí‚¹ ì—†ì´ ê³„ì† ì§„í–‰

        Args:
            sample_idx: ì™„ë£Œëœ ìƒ˜í”Œì˜ ë°°ì¹˜ ë‚´ ì¸ë±ìŠ¤
        """
        n_agent = getattr(self.config, 'n_agent', 8)
        prompt_idx = sample_idx // n_agent

        # í”„ë¡¬í”„íŠ¸ ID ì°¾ê¸°
        prompt_ids = list(self._prompt_completion_status.keys())
        if prompt_idx >= len(prompt_ids):
            return

        prompt_id = prompt_ids[prompt_idx]
        status = self._prompt_completion_status.get(prompt_id)
        if not status or status['submitted']:
            return

        status['completed_samples'] += 1

        # í”„ë¡¬í”„íŠ¸ì˜ ëª¨ë“  ìƒ˜í”Œì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if status['completed_samples'] >= status['total_samples']:
            # [Phase 6] ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì²˜ë¦¬ (ë¸”ë¡œí‚¹ ì—†ìŒ!)
            indices = list(status['sample_indices'])  # ë³µì‚¬ë³¸ ìƒì„±
            thread = threading.Thread(
                target=self._process_prompt_background,
                args=(indices, prompt_id, status),
                daemon=True,
                name=f"FrozenGen-{prompt_id}"
            )
            thread.start()
            self._pending_threads.append(thread)
            status['submitted'] = True  # ì¤‘ë³µ ì œì¶œ ë°©ì§€

            print(f"[Phase 6] í”„ë¡¬í”„íŠ¸ {prompt_id} ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹œì‘ "
                  f"(ìƒ˜í”Œ {len(indices)}ê°œ)")

    def _process_prompt_background(self, indices: List[int], prompt_id: str, status: dict):
        """
        [Phase 6] ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ Frozen Generator + Reward ì²˜ë¦¬

        ì´ í•¨ìˆ˜ëŠ” ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ë©”ì¸ ë£¨í”„ë¥¼ ë¸”ë¡œí‚¹í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

        Args:
            indices: ì²˜ë¦¬í•  ìƒ˜í”Œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
            prompt_id: í”„ë¡¬í”„íŠ¸ ID
            status: í”„ë¡¬í”„íŠ¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
        """
        try:
            start_time = _time.perf_counter()

            # 1. Frozen Generator í˜¸ì¶œ ì¤€ë¹„
            batch_questions = []
            batch_paths = []

            for i in indices:
                q = self.questions[i] if i < len(self.questions) else ""
                paths = self._prepare_generator_images(
                    self.retrievaled_images[i] if i < len(self.retrievaled_images) else [],
                    self.cropped_images[i] if i < len(self.cropped_images) else []
                )
                batch_questions.append(q)
                batch_paths.append(paths)

            # 2. Frozen Generator í˜¸ì¶œ (Phase 5 ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬)
            index2answer = self._call_frozen_generator_batch(
                indices, batch_questions, batch_paths
            )

            # 3. ê²°ê³¼ ì €ì¥ (Thread-safe)
            with self._thread_lock:
                for i in indices:
                    answer = index2answer.get(i, "")
                    self.generated_answers[i] = answer
                    self._streaming_frozen_generated.add(i)

            frozen_elapsed = _time.perf_counter() - start_time

            # 4. samples_data ìˆ˜ì§‘ (generated_answer í¬í•¨)
            samples_data = self._collect_samples_data(indices)

            # 5. Reward ì œì¶œ (Gemini VLM Judge í˜¸ì¶œ)
            self.streaming_reward_manager.submit_prompt(
                uid=prompt_id,
                sample_indices=indices,
                samples_data=samples_data
            )

            total_elapsed = _time.perf_counter() - start_time
            success_count = sum(1 for i in indices if self.generated_answers.get(i))

            print(f"[Phase 6] í”„ë¡¬í”„íŠ¸ {prompt_id} ì™„ë£Œ: "
                  f"Frozen={frozen_elapsed:.2f}s, Total={total_elapsed:.2f}s, "
                  f"Success={success_count}/{len(indices)}")

        except Exception as e:
            print(f"[Phase 6] í”„ë¡¬í”„íŠ¸ {prompt_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    def _collect_samples_data(self, indices: List[int]) -> List[Dict]:
        """
        [Phase 6] Reward ê³„ì‚°ì— í•„ìš”í•œ ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘

        ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œ RMManagerì— ì „ë‹¬í•  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        Phase 6ì—ì„œ generated_answer í•„ë“œê°€ ì¶”ê°€ë˜ì–´ Gemini VLM Judgeê°€
        ì˜¬ë°”ë¥´ê²Œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        Args:
            indices: ìˆ˜ì§‘í•  ìƒ˜í”Œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸

        Returns:
            ê° ìƒ˜í”Œì˜ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        samples_data = []

        # [FIX] gen_batchì—ì„œ ground_truth ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        gen_batch = getattr(self, '_current_gen_batch', None)
        ground_truths = []
        reference_image_paths_list = []
        reference_basenames_list = []

        if gen_batch is not None:
            try:
                # reward_modelì—ì„œ ground_truth ê°€ì ¸ì˜¤ê¸°
                reward_model_data = gen_batch.non_tensor_batch.get('reward_model', {})
                if isinstance(reward_model_data, dict):
                    ground_truths = reward_model_data.get('ground_truth', [])
                elif hasattr(reward_model_data, '__iter__'):
                    # numpy array ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                    ground_truths = [
                        item.get('ground_truth', '') if isinstance(item, dict) else str(item)
                        for item in reward_model_data
                    ]

                # extra_infoì—ì„œ reference ì´ë¯¸ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                extra_infos = gen_batch.non_tensor_batch.get('extra_info', [])
                for info in extra_infos:
                    if isinstance(info, dict):
                        ref_paths = info.get('reference_image_paths', [])
                        reference_image_paths_list.append(ref_paths)
                        reference_basenames_list.append([
                            os.path.basename(p.rstrip('/')).split(".jpg")[0]
                            for p in ref_paths
                        ])
                    else:
                        reference_image_paths_list.append([])
                        reference_basenames_list.append([])
            except Exception as e:
                print(f"[WARNING] ground_truth ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        for idx in indices:
            # ê²€ìƒ‰ëœ ì´ë¯¸ì§€ ê²½ë¡œ
            retrieved_images = list(self.retrievaled_images[idx]) if idx < len(self.retrievaled_images) else []

            # NDCG ê³„ì‚°ìš© basename ì¶”ì¶œ
            retrieved_basenames = [
                os.path.basename(p.rstrip('/')).split(".jpg")[0]
                for p in retrieved_images
            ]

            # ì§ˆë¬¸ ì¶”ì¶œ
            question = self.questions[idx] if idx < len(self.questions) else ''

            # [Phase 6] Frozen Generatorì—ì„œ ìƒì„±ëœ ë‹µë³€
            generated_answer = self.generated_answers.get(idx, '') if hasattr(self, 'generated_answers') else ''

            # [FIX] reference_answer ì¶”ì¶œ (ground_truthì—ì„œ)
            reference_answer = ''
            if idx < len(ground_truths):
                gt = ground_truths[idx]
                reference_answer = gt if isinstance(gt, str) else str(gt) if gt else ''

            # [FIX] reference ì´ë¯¸ì§€ ì •ë³´
            ref_img_paths = reference_image_paths_list[idx] if idx < len(reference_image_paths_list) else []
            ref_basenames = reference_basenames_list[idx] if idx < len(reference_basenames_list) else []

            samples_data.append({
                'query': question,
                'retrieved_images': retrieved_images,
                'retrieved_basenames': retrieved_basenames,
                'generated_answer': generated_answer,  # [Phase 6] NEW!
                'response_str': '',
                'reference_answer': reference_answer,  # [FIX] ground_truthì—ì„œ ê°€ì ¸ì˜´
                'reference_image_paths': ref_img_paths,
                'reference_basenames': ref_basenames,
            })

        return samples_data
