import os
import torch
from collections import defaultdict
from transformers import AutoConfig, AutoModelForVision2Seq, AutoProcessor, AutoTokenizer

# DTensor í¬ì¥ ë²—ê¸°ê¸° (ì´ì „ê³¼ ë™ì¼)
def unwrap_dtensor(tensor):
    if type(tensor).__name__ == 'DTensor' or hasattr(tensor, 'to_local'):
        return tensor.to_local()
    return tensor

def merge_grpo_checkpoint(base_path, output_path):
    config_path = os.path.join(base_path, "actor/huggingface")
    
    # 1. ì„¤ì • ë¡œë“œ
    print(f"ğŸ› ï¸ [1ë‹¨ê³„] Config ë¡œë“œ: {config_path}")
    try:
        config = AutoConfig.from_pretrained(config_path, trust_remote_code=True)
    except Exception as e:
        print(f"   âŒ Config ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 2. ë¹ˆ ëª¨ë¸ ìƒì„±
    print(f"ğŸ—ï¸ [2ë‹¨ê³„] ë¹ˆ ëª¨ë¸ ìƒì„± ì¤‘...")
    with torch.device("cpu"):
        model = AutoModelForVision2Seq.from_config(config, trust_remote_code=True)
    model.to(dtype=torch.bfloat16)

    # 3. ê°€ì¤‘ì¹˜ ìˆ˜ì§‘ (ì´ì–´ë¶™ì´ê¸° ì¤€ë¹„)
    # [ìˆ˜ì •ë¨] 4ê°œ -> 2ê°œë¡œ ë¡œê·¸ ë©”ì‹œì§€ ë³€ê²½
    print("ğŸ§© [3ë‹¨ê³„] 2ê°œ GPUì˜ íŒŒë¼ë¯¸í„° ì¡°ê°ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ë©”ëª¨ë¦¬ ì£¼ì˜)...")
    
    shards = defaultdict(list)
    
    # [ìˆ˜ì •ë¨] range(4) -> range(2)ë¡œ ë³€ê²½ (0, 1ë²ˆë§Œ ë°˜ë³µ)
    for rank in range(2):
        # [ìˆ˜ì •ë¨] íŒŒì¼ëª… í¬ë§· ë³€ê²½: world_size_4 -> world_size_2
        checkpoint_name = f"model_world_size_2_rank_{rank}.pt"
        checkpoint_path = os.path.join(base_path, "actor", checkpoint_name)
        
        print(f"   ã„´ ğŸ“‚ Rank {rank} ë¡œë“œ ì¤‘...")
        if not os.path.exists(checkpoint_path):
            print(f"      âŒ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
            return

        state_dict = torch.load(checkpoint_path, map_location="cpu")
        
        for key, tensor in state_dict.items():
            # DTensor í¬ì¥ ë²—ê¸°ê¸°
            clean_tensor = unwrap_dtensor(tensor)
            shards[key].append(clean_tensor)

    # 4. ì¡°ê° ì´ì–´ë¶™ì´ê¸° (Concatenate)
    print("âœ¨ [4ë‹¨ê³„] ìˆ˜ì§‘ëœ ì¡°ê°ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤ (Concatenate)...")
    full_state_dict = {}
    
    for key, tensor_list in shards.items():
        try:
            # ë¡œê·¸ë¥¼ ë³¼ ë•Œ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ 0ë²ˆ ì°¨ì›(dim=0)ìœ¼ë¡œ ìª¼ê°œì ¸ ìˆìŒ
            merged_tensor = torch.cat(tensor_list, dim=0)
            full_state_dict[key] = merged_tensor
        except Exception as e:
            print(f"   âš ï¸ ë³‘í•© ì‹¤íŒ¨ ({key}): {e} -> ì²« ë²ˆì§¸ ì¡°ê°ë§Œ ì‚¬ìš© ì‹œë„")
            full_state_dict[key] = tensor_list[0]

    # 5. ëª¨ë¸ì— ì£¼ì…
    print("ğŸ’‰ [5ë‹¨ê³„] ì™„ì„±ëœ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ì— ì£¼ì…í•©ë‹ˆë‹¤...")
    
    missing, unexpected = model.load_state_dict(full_state_dict, strict=False)
    
    if missing:
        print(f"   âš ï¸ ëˆ„ë½ëœ í‚¤: {len(missing)}ê°œ")
    if unexpected:
        print(f"   â„¹ï¸ ë¶ˆí•„ìš”í•œ í‚¤: {len(unexpected)}ê°œ")

    # 6. ì €ì¥
    print(f"ğŸ’¾ [6ë‹¨ê³„] '{output_path}'ì— ì €ì¥ ì¤‘...")
    model.save_pretrained(output_path)
    
    try:
        processor = AutoProcessor.from_pretrained(config_path, trust_remote_code=True)
        processor.save_pretrained(output_path)
    except:
        tokenizer = AutoTokenizer.from_pretrained(config_path, trust_remote_code=True)
        tokenizer.save_pretrained(output_path)

    print(f"âœ… ë³‘í•© ì™„ë£Œ! í™•ì¸í•´ë³´ì„¸ìš”: {output_path}")

# --- ì‹¤í–‰ë¶€ ---
input_folder = "/data/daedong/gspo_phase1/gspo_phase1_last" 
output_folder = "./RL_results/merged_gspo_phase1"

if __name__ == "__main__":
    merge_grpo_checkpoint(input_folder, output_folder)