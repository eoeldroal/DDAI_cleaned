# Bucket 0 데이터 품질 분석 및 필터링 보고서

## 요약

본 문서는 SlideVQA 커리큘럼 러닝 파이프라인의 Bucket 0 데이터셋에 대한 종합적인 품질 분석 결과를 제공한다. Bucket 0은 모델이 NDCG 점수 0.1 미만을 기록한 샘플들로, 데이터셋에서 가장 어려운 사례들을 포함한다. 전체 637개 샘플에 대해 원본 이미지를 직접 확인하는 체계적인 수동 검수를 수행한 결과, 55개 샘플(8.6%)에서 강화학습에 부정적 영향을 미칠 수 있는 데이터 품질 문제를 식별하였다. 이 필터링 과정을 통해 후속 집중 강화학습이 유효하고 학습 가능한 샘플에서만 수행될 수 있도록 보장한다.

---

## 1. 서론

### 1.1 연구 배경

시각적 질의응답(VQA) 시스템을 위한 커리큘럼 러닝에서, 학습 샘플은 모델 성능 지표에 기반하여 난이도별로 분류된다. GSPO(Generalized Self-Play Optimization) Phase 1 학습은 각 샘플에 대한 NDCG(Normalized Discounted Cumulative Gain) 점수를 산출하였으며, 이를 통해 세 개의 버킷으로 계층화가 가능하였다:

| 버킷 | NDCG 범위 | 샘플 수 | 설명 |
|------|-----------|---------|------|
| B (Mastered) | > 0.7 | 2,791개 | 모델이 잘 수행함; 추가 학습에서 제외 |
| A (Edge-of-Competence) | 0.1 - 0.7 | 1,560개 | 최적의 학습 영역; 주요 학습 대상 |
| 0 (Unsolvable) | < 0.1 | 637개 | 모델이 지속적으로 실패함; 조사 필요 |

### 1.2 연구 동기

Bucket 0 샘플은 핵심적인 질문을 제기한다: **왜 모델이 실패하는가?**

두 가지 가설이 존재한다:

1. **진정한 난이도**: 샘플이 본질적으로 어렵지만, 더 많은 계산 자원(롤아웃, 턴, 온도)을 투입하면 학습 가능함
2. **데이터 품질 문제**: 샘플에 어노테이션 오류, 참조 불일치, 또는 모호한 정답이 포함되어 있어 학습 자체가 불가능함

데이터 품질 문제가 있는 샘플로 학습하면:
- 불가능한 과제에 계산 자원 낭비
- 보상 신호에 노이즈 도입
- 보상 해킹(reward hacking) 또는 정책 성능 저하 유발 가능

### 1.3 연구 목표

1. 637개 Bucket 0 샘플 전수 검사
2. 원본 이미지 확인을 통한 정답 정확성 검증
3. 데이터 품질 문제 식별 및 분류
4. 집중 강화학습에 적합한 필터링된 데이터셋 생성

---

## 2. 연구 방법론

### 2.1 데이터 구조

데이터셋의 각 샘플은 다음 구조를 가진다:

```python
{
    'id': 'train_XXXX',           # 고유 식별자
    'file_name': 'XXXX.pdf',      # 원본 문서
    'reference_page': [N, M],     # 정답이 포함된 페이지
    'prompt': [...],              # 질문이 포함된 대화 형식
    'reward_model': {
        'ground_truth': '...',    # 기대 정답
        'style': '...'            # 정답 형식
    },
    'extra_info': {
        'phase1_ndcg': 0.0XX      # Phase 1의 NDCG 점수
    }
}
```

### 2.2 이미지 검증 과정

이미지는 다음 경로에 저장되어 있다:
```
/search_engine/corpus/img/{doc_id}_{page}.jpg
```

예를 들어, `file_name="73.pdf"`이고 `reference_page=[1, 2]`인 경우:
- `73_1.jpg`
- `73_2.jpg`

### 2.3 필터링 기준

데이터 품질 문제를 네 가지 범주로 구분하였다:

#### 범주 1: 정답 오류 (Ground Truth Error)
어노테이션된 정답이 원본 이미지와 비교했을 때 사실과 다른 경우.

**탐지 방법**: 참조 이미지를 직접 시각적으로 검사하여 정답 값을 검증.

#### 범주 2: 질문-이미지 불일치 (Question-Image Mismatch)
참조된 이미지가 질문과 관련된 정보를 포함하지 않는 경우.

**탐지 방법**: 질문의 주제가 참조 페이지에 나타나는지 확인.

#### 범주 3: 질문-답변 유형 불일치 (Question-Answer Type Mismatch)
답변 유형이 질문이 요구하는 것과 맞지 않는 경우.

**탐지 방법**: 질문 의도와 답변 형식의 의미적 분석.

#### 범주 4: 불완전하거나 모호한 답변
정답이 잘리거나, 문법적으로 불완전하거나, 신뢰할 수 있는 평가가 어려울 정도로 모호한 경우.

**탐지 방법**: 답변 완전성의 언어학적 분석.

### 2.4 검수 프로토콜

1. **병렬 처리**: 8개의 검수 에이전트, 각각 약 80개 샘플 담당
2. **전수 조사**: 637개 샘플 모두 개별 검사
3. **이미지 검증**: 의심스러운 샘플은 실제 이미지 파일을 직접 읽어 확인
4. **통계적 접근 배제**: 패턴이 아닌 내용 중심으로 모든 샘플 검토

---

## 3. NDCG 분포 분석

상세 검수 전, Bucket 0 내의 NDCG 점수 분포를 분석하였다:

| NDCG 범위 | 샘플 수 | 비율 |
|-----------|---------|------|
| 0.0625 - 0.0990 | 80개 | 12.6% |
| 0.0383 - 0.0625 | 80개 | 12.6% |
| 정확히 0.0000 | 477개 | 74.9% |

**핵심 발견**: Bucket 0 샘플의 75%가 NDCG = 0으로, 완전한 검색 실패를 나타낸다. 이는 해당 샘플들이 검색을 극도로 어렵게 만드는 본질적 특성을 가질 수 있음을 시사한다.

---

## 4. 상세 분석 결과

### 4.1 요약 통계

| 지표 | 값 |
|------|-----|
| 검수된 총 샘플 수 | 637개 |
| 문제가 있는 샘플 수 | 55개 |
| 문제 발생률 | 8.6% |
| 정상 샘플 수 | 582개 |
| 정상 비율 | 91.4% |

### 4.2 문제 유형별 분포

| 범주 | 샘플 수 | 문제 중 비율 |
|------|---------|-------------|
| 질문-이미지 불일치 | 32개 | 58.2% |
| 정답 오류 | 19개 | 34.5% |
| 질문-답변 유형 불일치 | 4개 | 7.3% |

### 4.3 인덱스 범위별 문제 분포

| 인덱스 범위 | 샘플 수 | 문제 수 | 문제 발생률 |
|-------------|---------|---------|-------------|
| 0-79 | 80개 | 4개 | 5.0% |
| 80-159 | 80개 | 3개 | 3.8% |
| 160-239 | 80개 | 7개 | 8.8% |
| 240-319 | 80개 | 11개 | 13.8% |
| 320-399 | 80개 | 13개 | 16.3% |
| 400-479 | 80개 | 3개 | 3.8% |
| 480-559 | 80개 | 10개 | 12.5% |
| 560-636 | 77개 | 4개 | 5.2% |

**관찰**: 문제 발생률이 인덱스 범위에 따라 크게 변동(3.8% ~ 16.3%)하며, 이는 원본 데이터셋의 어노테이션 품질이 균일하지 않음을 시사한다.

---

## 5. 상세 오류 분석

### 5.1 정답 오류 (19건)

이는 어노테이션된 정답이 사실과 다른 가장 심각한 데이터 품질 문제를 나타낸다.

#### 5.1.1 수치 오류

| ID | 질문 | 정답(GT) | 실제 값 | 오류 유형 |
|----|------|---------|---------|-----------|
| train_1828 | "y축 최고값과 최저값의 차이는?" | 100000 | 10000 | 자릿수 오류 |
| train_6996 | "지출이 몇 조 달러 증가했나?" | 2.8 | 2.5 | 잘못된 읽기 |
| train_8020 | "2006년이 2005년보다 몇 백만 건 더 많나?" | 0.9 | 0.36 | 계산 오류 |
| train_6885 | "상악동에 몇 개의 골벽이 있나?" | 14 | 6 | 오독 |
| train_2024 | "파이 차트에 몇 개의 조각이 있나?" | 9 | 11 | 카운팅 오류 |
| train_3242 | "침대당 최소 린넨 세트 수는?" | 4 | 6 | 잘못된 값 |

#### 5.1.2 사실 오류

| ID | 질문 | 정답(GT) | 올바른 답변 | 오류 설명 |
|----|------|---------|------------|-----------|
| train_8538 | "B2C란 무엇인가?" | business-to-computer | business-to-consumer | 근본적 용어 오류 |
| train_6419 | "핵에는 무엇이 포함되어 있나?" | protons and electrons | protons and neutrons | 과학적 오류 |
| train_8910 | "Luxottica가 인수에 얼마를 투자했나?" | 308 million | 44 million | 배당금과 혼동 |

**사례 연구: train_8538 (B2C 정의)**

```
질문: "B2C란 무엇인가?"
정답(GT): "business-to-computer"
이미지 내용: 슬라이드에 "Business-to-consumer (B2C)"가 명확히 표시됨

분석: 이것은 근본적인 용어 오류이다. B2C는 전자상거래 맥락에서
보편적으로 "Business-to-Consumer"(기업과 소비자 간 거래)를 의미한다.
"business-to-computer"라는 어노테이션은 명백히 틀렸다.

영향: 학습에 포함될 경우, 모델이 올바른 답변을 했을 때 오히려
패널티를 받게 되어 보상 신호가 왜곡된다.
```

**사례 연구: train_6419 (원자 구조)**

```
질문: "핵에는 무엇이 포함되어 있나?"
정답(GT): "protons and electrons"
이미지 내용: 교육용 슬라이드에 "Nucleus contains protons and neutrons" 명시

분석: 기초 원자 물리학 - 전자는 핵 주위를 공전하지만 핵 안에
포함되어 있지 않다. 핵은 양성자와 중성자를 포함한다.

영향: 이 샘플로 학습하면 모델에게 잘못된 과학적 사실을 가르치게 된다.
```

#### 5.1.3 오타 오류

| ID | 질문 | 정답(GT) | 실제 값 | 오류 |
|----|------|---------|---------|------|
| train_10185 | "분데스리가 평균 경기장 수용 인원은?" | 46746 seats | 46747 seats | 1 차이 |
| train_7835 | "발표자가 Senior Experience Designer로 근무하는 곳은?" | Honeywell User Design | Honeywell User Experience | 단어 치환 |
| train_8374 | "Sonia Simone의 웹페이지 URL은?" | my.copybloggee.com | my.copyblogger.com | 오타 (bloggee vs blogger) |
| train_288 | "사용된 마더보드는?" | 297 MPOWER MAX | Z97 MPOWER MAX | OCR 오류 (Z→2) |
| train_8288 | "Pris의 IP 주소는?" | 192.168.7.8:9205 | 192.168.7.8:9204 | 포트 번호 오류 |

### 5.2 질문-이미지 불일치 (32건)

참조된 페이지가 질문에 답하는 데 필요한 정보를 포함하지 않는 경우이다.

#### 5.2.1 체계적 불일치 패턴

**패턴 A: VAT Act 시리즈 (5개 샘플)**

| ID | 질문 주제 | 참조 이미지 실제 내용 |
|----|-----------|----------------------|
| train_7311 | "부가세가 어느 법령에 의해 부과되나?" | "Isteak Ahmed 1478" (이름 슬라이드) |
| train_7310 | "부가세는 얼마인가?" | 동일한 무관한 내용 |
| train_7309 | "섹션 7은 어떤 종류의 세금을 부과하나?" | 동일한 무관한 내용 |
| train_7308 | "VAT 적용 최소 매출액은?" | 동일한 무관한 내용 |
| train_7307 | "어떤 섹션이 매출세를 다루나?" | 동일한 무관한 내용 |

**분석**: 다섯 개 샘플 모두 문서 7307-7311의 페이지를 참조하는데, 해당 페이지에는 VAT Act 내용이 아닌 발표자 이름만 포함되어 있다. 이는 reference_page 어노테이션의 체계적 오류를 시사한다.

**패턴 B: Dreamy Draw Nature Trail 시리즈 (3개 샘플)**

| ID | 질문 | 참조 이미지 |
|----|------|-----------|
| train_7226 | "Dreamy Draw Nature Trail은 언제 이용 가능한가?" | 건강보험 설문 질문 |
| train_7225 | "Dreamy Draw Nature Trail의 길이는?" | 동일한 건강보험 내용 |
| train_7224 | "Dreamy Draw Nature Trail은 어느 주에 있나?" | 동일한 건강보험 내용 |

**분석**: 하이킹 트레일에 관한 세 개의 연속 샘플이 모두 동일한 무관한 건강보험 설문 슬라이드를 참조한다.

**패턴 C: Courtesy Chevrolet 시리즈 (2개 샘플)**

| ID | 질문 | 참조 이미지 |
|----|------|-----------|
| train_2137 | "y축의 최저값은?" (Courtesy Chevrolet 차트) | "History of Regression" 슬라이드 |
| train_2135 | "x축에서 가장 높은 두 값의 차이는?" | 동일한 무관한 내용 |

#### 5.2.2 개별 불일치 사례

| ID | 질문 주제 | 예상 내용 | 실제 이미지 내용 |
|----|-----------|----------|-----------------|
| train_7035 | 2013년 3분기 회사별 RPM | 수익/가격 데이터 | 구독자 성장 테이블 |
| train_10194 | 트위터 팔로우 비율 | 소셜미디어 통계 | 페이스북 페이지 방문 통계만 |
| train_7968 | 정당별 언급 비율 | 선거 데이터 | 뉴스 기사 스크린샷 |
| train_6921 | "Superintelligence" 저자 | 책/저자 정보 | 일자리 통계가 있는 로봇 이미지 |
| train_6944 | 2015-2016 미국 모바일 광고 지출 | 광고 지출 데이터 | RTB 경매 프로세스 다이어그램 |
| train_5777 | "파이 차트에서 3%를 차지하는 것은?" | 파이 차트 데이터 | FIFA 스폰서십 피라미드 |

### 5.3 질문-답변 유형 불일치 (4건)

| ID | 질문 | 예상 답변 유형 | 정답(GT) | 문제점 |
|----|------|---------------|---------|--------|
| train_9056 | "10억 달러 분기를 달성한 최초의 클라우드 컴퓨팅 회사는?" | 회사명 | "Q314" | 회사 대신 분기 (정답은 "Salesforce") |
| train_8827 | "애자일 개발의 두 가지 모델은?" | 애자일 방법론 | "Waterfall, Spiral models" | 이것들은 전통적(비애자일) 모델임 |
| train_9377 | "2015년에 출시된 피처폰 수는?" | 피처폰 수 | "405" | 데이터는 피처폰이 아닌 스마트폰에 관한 것 |

**사례 연구: train_8827 (애자일 개발 모델)**

```
질문: "애자일 개발의 두 가지 모델은 무엇인가?"
정답(GT): "Waterfall, Spiral models"
이미지 내용: "The Waterfall Model"과 "The Spiral Model"이 표시됨

분석: 이것은 의미적 불일치를 나타낸다. Waterfall과 Spiral은
명시적으로 애자일 개발 모델이 아니다 - 이것들은 애자일이
대체하기 위해 설계된 전통적 소프트웨어 개발 방법론이다.

질문은 애자일 모델(예: Scrum, Kanban, XP)을 묻지만,
정답은 비애자일 모델을 제공한다.

영향: 의미 이해를 사용하는 LLM 심판은 이 애자일 방법에 관한
질문에 대해 "Waterfall, Spiral" 답변을 올바르게 거부할 것이다.
```

---

## 6. 필터링된 데이터셋

### 6.1 제외된 샘플 ID 전체 목록

```python
excluded_ids = [
    # 인덱스 0-79 (4개 샘플)
    "train_7035", "train_1828", "train_10194", "train_7968",

    # 인덱스 80-159 (3개 샘플)
    "train_10445", "train_6996", "train_8020",

    # 인덱스 160-239 (7개 샘플)
    "train_10185", "train_7835", "train_9377", "train_7439",
    "train_7438", "train_7352", "train_7494",

    # 인덱스 240-319 (11개 샘플)
    "train_7685", "train_7684", "train_7513", "train_7564",
    "train_7538", "train_7514", "train_8538", "train_8910",
    "train_8374", "train_8827", "train_8922",

    # 인덱스 320-399 (13개 샘플)
    "train_9056", "train_8105", "train_8288", "train_8281",
    "train_9667", "train_8259", "train_8228", "train_8226",
    "train_7311", "train_2602", "train_2137", "train_2135", "train_2024",

    # 인덱스 400-479 (3개 샘플)
    "train_3242", "train_288", "train_7310",

    # 인덱스 480-559 (10개 샘플)
    "train_6885", "train_6419", "train_6687", "train_6921",
    "train_7309", "train_7308", "train_7307", "train_7226",
    "train_7225", "train_7224",

    # 인덱스 560-636 (4개 샘플)
    "train_6944", "train_5777", "train_4515", "train_5774"
]
```

### 6.2 출력 파일

| 파일 | 샘플 수 | 설명 |
|------|---------|------|
| `curriculum_bucket_0_filtered.parquet` | 582개 | 집중 RL을 위한 정상 샘플 |
| `curriculum_bucket_0_excluded.parquet` | 55개 | 문제 있는 샘플 (분석용) |

### 6.3 필터링 코드

```python
import pandas as pd

# 원본 데이터 로드
df = pd.read_parquet('data/curriculum_bucket_0.parquet')

# 제외 목록 정의
filter_ids = [...]  # 위에 나열된 55개 ID

# 필터 적용
df_filtered = df[~df['id'].isin(filter_ids)]
df_excluded = df[df['id'].isin(filter_ids)]

# 저장
df_filtered.to_parquet('data/curriculum_bucket_0_filtered.parquet', index=False)
df_excluded.to_parquet('data/curriculum_bucket_0_excluded.parquet', index=False)
```

---

## 7. LLM-as-Judge에 대한 시사점

### 7.1 오류 범주별 심판 견고성

| 오류 유형 | LLM 심판이 처리 가능? | 근거 |
|-----------|----------------------|------|
| 경미한 오타 (bloggee vs blogger) | 가능 | 의미적 유사성 탐지 |
| 1 차이 수치 오류 (46746 vs 46747) | 가능 | 근사 매칭 |
| 사실 오류 (B2C 정의) | **불가능** | 올바른 답변에 패널티 부여 |
| 질문-이미지 불일치 | **불가능** | 정답을 얻을 수 없음 |
| 의미적 불일치 (애자일 모델) | 부분적 가능 | 심판 프롬프트 설계에 따라 다름 |

### 7.2 안전한 사용 통계

필터링 후:
- **582 / 637 (91.4%)** 샘플이 LLM-as-Judge 평가에 안전하게 사용 가능
- 남은 샘플들은 원본 이미지에 대해 검증 가능한 유효한 질문-답변 쌍을 가짐

---

## 8. 권장사항

### 8.1 집중 RL 학습(DDAI-47)을 위한 권장사항

1. `curriculum_bucket_0_filtered.parquet` (582개 샘플)을 입력으로 사용
2. 이 샘플들은 진정으로 어렵지만 학습 가능한 사례를 나타냄
3. 학습 효율을 최대화하기 위해 컴퓨트 커리큘럼(Small → Medium → Large) 적용

### 8.2 향후 데이터셋 큐레이션을 위한 권장사항

1. 자동화된 reference_page 검증 구현
2. OCR 추출 텍스트와 정답 교차 검증
3. 단일 단어 답변 샘플을 수동 검토 대상으로 표시

### 8.3 어노테이션 가이드라인을 위한 권장사항

1. 권위 있는 출처에 대해 사실 주장 검증
2. 질문-답변 유형 정렬 보장
3. QA 워크플로우에 페이지 수준 내용 검증 포함

---

## 9. 결론

637개 Bucket 0 샘플 전체에 대한 직접 이미지 검증을 포함한 체계적인 수동 검수를 통해, 55개 샘플(8.6%)에서 데이터 품질 문제를 식별하였다. 주요 문제 유형은 질문-이미지 불일치(58.2%)였으며, 정답 오류(34.5%)가 그 뒤를 이었다.

582개의 필터링된 데이터셋은 어노테이션 오류가 아닌 과제 난이도로 인해 모델이 실패한 진정으로 도전적인 사례들을 나타낸다. 이 정제된 데이터셋은 증가된 계산 자원을 활용한 집중 강화학습에 적합하다.

---

## 부록 A: 검수 방법론 상세

### A.1 병렬 에이전트 구성

- **총 에이전트 수**: 8개
- **에이전트당 샘플 수**: 약 80개
- **인덱스 범위**: 0-79, 80-159, 160-239, 240-319, 320-399, 400-479, 480-559, 560-636

### A.2 샘플별 검수 프로토콜

```
각 샘플에 대해:
1. 프롬프트에서 질문 추출 (role='user' 콘텐츠)
2. reward_model에서 ground_truth 읽기
3. 이미지 경로 구성: {doc_id}_{page}.jpg
4. 이미지를 열어 시각적으로 검사
5. 이미지 내용과 정답 일치 여부 검증
6. 질문-답변 유형 정렬 확인
7. 문제 발견 시 표시
8. 이미지에서 확인한 증거와 함께 구체적 문제 문서화
```

### A.3 품질 보증

- 통계적 샘플링 없이 100% 전수 조사
- 표시된 각 샘플은 이미지 검사의 구체적 증거 포함
- 체계적 패턴 교차 참조 (예: VAT Act 시리즈)

---

## 부록 B: 오류 범주 정의

### B.1 정답 오류

**정의**: 어노테이션된 답변이 참조 이미지에서 보이는 정보와 비교했을 때 사실적으로 틀린 경우.

**하위 범주**:
- 수치 오류 (잘못된 값, 계산 실수)
- 사실 오류 (틀린 용어, 과학적 오류)
- 오타 오류 (OCR 실수, 문자 치환)

### B.2 질문-이미지 불일치

**정의**: 참조된 페이지가 질문에 답하는 데 필요한 정보를 포함하지 않는 경우.

**지표**:
- 이미지 내용이 질문 주제와 무관
- 참조된 어떤 페이지에서도 필요한 데이터가 보이지 않음
- 동일 문서의 여러 샘플에서 체계적 불일치

### B.3 질문-답변 유형 불일치

**정의**: 정답의 답변 유형이 질문의 의미적 기대와 맞지 않는 경우.

**예시**:
- 질문이 "어느 회사?"를 묻는데 → 답변이 날짜를 제공
- 질문이 "애자일 방법"에 대해 묻는데 → 답변이 비애자일 방법을 나열

---

## 부록 C: 파일 검증

```
curriculum_bucket_0.parquet          (원본)       637개 샘플
curriculum_bucket_0_filtered.parquet (정제됨)     582개 샘플
curriculum_bucket_0_excluded.parquet (문제있음)    55개 샘플
```

**검증**:
```python
assert len(df_filtered) + len(df_excluded) == len(df_original)  # 582 + 55 = 637
assert len(set(df_filtered['id']) & set(df_excluded['id'])) == 0  # 중복 없음
```

---

*문서 생성일: 2024-12-28*
*분석 수행: Claude를 활용한 체계적 이미지 검증*
