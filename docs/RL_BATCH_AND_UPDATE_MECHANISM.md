# GSPO/PPO 배치 및 업데이트 메커니즘 완벽 가이드

> **목적**: `train_batch_size`, `n_agent`, `mini_batch`, `micro_batch` 등 헷갈리기 쉬운 강화학습(RL) 배치 설정들의 관계와, 실제 학습이 일어나는 내부 과정을 시각적 예시와 함께 아주 자세히 설명합니다.
>
> **대상**: DDAI 프로젝트의 GSPO/PPO 학습 스크립트를 사용하는 엔지니어
> **작성일**: 2025-12-26

---

## 목차

1. [핵심 요약](#1-핵심-요약)
2. [용어 정의 및 설정값](#2-용어-정의-및-설정값)
3. [전체 데이터 흐름도](#3-전체-데이터-흐름도)
4. [상세 과정 분석](#4-상세-과정-분석)
    - [Step 1: 데이터 수집 (Rollout)](#step-1-데이터-수집-rollout)
    - [Step 2: Advantage 계산 (GRPO)](#step-2-advantage-계산-grpo)
    - [Step 3: 학습 루프 (PPO Update)](#step-3-학습-루프-ppo-update)
5. [메모리와 OOM 분석](#5-메모리와-oom-분석)
6. [학습 안정성 (32번 업데이트의 비밀)](#6-학습-안정성-32번-업데이트의-비밀)

---

## 1. 핵심 요약

- **데이터 수집량**: 한 번의 Rollout으로 **512개**의 데이터를 모읍니다. (`train_batch_size=64` × `n_agent=8`)
- **업데이트 횟수**: 이 512개 데이터를 한 번에 학습하지 않고, **16개씩(mini-batch)** 나누어 총 **32번** 업데이트합니다.
- **메모리 안전장치**: GPU는 이 16개를 한 번에 올리지 않고, **1개씩(micro-batch)** 순차적으로 처리하여 메모리 폭발(OOM)을 막습니다.
- **결론**: **"데이터는 많이 모으고(512개), 학습은 조금씩 자주(32번), 연산은 하나씩 차근차근(1개씩)"** 하는 구조입니다.

---

## 2. 용어 정의 및 설정값

다음은 `gspo_phase2_gemini_flash.sh` 기준 설정입니다. (GPU 4대 가정)

| 설정 변수 | 예시 값 | 의미 | 비유 (카드 게임) |
|---|---|---|---|
| `train_batch_size` | **64** | 한 번에 사용할 질문(프롬프트)의 개수 | 이번 판에 사용할 문제 카드 64장 |
| `n_agent` | **8** | 질문 하나당 생성할 답변의 개수 | 문제당 8명의 플레이어가 답을 제출 |
| **Total Rollout** | **512** | 총 수집되는 데이터 개수 (64 × 8) | 총 512장의 답안지 뭉치 |
| `ppo_mini_batch_size` | **16** | **한 번의 Weight Update**에 사용할 데이터 수 | 채점관이 한 번에 책상에 올리는 답안지 16장 |
| `ppo_micro_batch_size_per_gpu` | **1** | **GPU 메모리에 동시에 올리는** 데이터 수 | 채점관이 손에 들고 읽는 답안지 1장 |
| `ppo_epochs` | **1** | 같은 데이터로 **몇 번 반복** 학습할지 (기본값: 1) | 답안지 뭉치를 처음부터 끝까지 몇 번 다시 볼지 |

> ⚠️ **총 업데이트 횟수** = `(Total Rollout / mini_batch) × ppo_epochs` = (512 / 16) × 1 = **32번**

---

## 3. 전체 데이터 흐름도

```
[Phase 1: Rollout - 데이터 수집]
┌──────────────────────────────────────────────┐
│ Prompt 1 ──▶ [답변1, 답변2, ..., 답변8]      │
│ Prompt 2 ──▶ [답변1, 답변2, ..., 답변8]      │
│ ...                                          │
│ Prompt 64 ──▶ [답변1, 답변2, ..., 답변8]     │
└──────────────────────┬───────────────────────┘
                       │ 총 512개 데이터 생성
                       ▼
[Phase 2: GRPO Advantage - 그룹 평가]
┌──────────────────────────────────────────────┐
│ "너는 같은 그룹 8명 평균보다 얼마나 잘했니?" │
│ (이 단계에서만 그룹 정보 사용)               │
└──────────────────────┬───────────────────────┘
                       │ Advantage 점수 부여 완료
                       ▼
[Phase 3: Flatten - 독립 데이터화]
┌──────────────────────────────────────────────┐
│ 512개의 낱개 데이터로 분해                   │
│ [D_0, D_1, D_2, ..., D_511]                  │
│ ⚠️ FSDP 환경에서는 shuffle 없이 순차 처리    │
│ (Megatron 환경에서만 shuffle 지원)           │
└──────────────────────┬───────────────────────┘
                       │
                       ▼
[Phase 4: PPO Update - 32번 × ppo_epochs 반복]
┌──────────────────────────────────────────────┐
│ Loop 1: 16개 뽑아서 학습 (Update!)           │
│ Loop 2: 16개 뽑아서 학습 (Update!)           │
│ ...                                          │
│ Loop 32: 16개 뽑아서 학습 (Update!)          │
└──────────────────────────────────────────────┘
```

---

## 4. 상세 과정 분석

### Step 1: 데이터 수집 (Rollout)

- **설정**: `train_batch_size=64`, `n_agent=8`
- **동작**: 64개의 프롬프트를 8번씩 복제하여 총 512개의 시퀀스를 생성합니다.
- **결과**: 512개의 `(Question, Image, Answer)` 쌍이 메모리(RAM)에 저장됩니다.

### Step 2: Advantage 계산 (GRPO)

- **핵심**: GRPO는 **"상대 평가"**입니다.
- **동작**: 같은 질문에서 나온 8개의 답변끼리 보상(Reward)을 비교합니다.
    - 예: 질문 A에 대한 답변들의 점수가 `[0.1, 0.2, 0.8, 0.9, ...]`라면, 평균은 `0.5`입니다.
    - 0.9점 받은 답변은 `(0.9 - 0.5) / std` 만큼의 **Advantage(가산점)**를 받습니다.
- **중요**: 이 계산이 끝나면, "그룹"의 의미는 사라집니다. 이제 각 데이터는 독립적인 `(Input, Output, Advantage)` 셋이 됩니다.

### Step 3: 학습 루프 (PPO Update)

이제 512개의 데이터를 가지고 모델을 학습시킵니다. **총 32번의 업데이트**가 일어납니다.

#### Q. 왜 32번인가요?
> 전체 데이터 512개 / 미니배치 16개 = **32 덩어리**

#### 상세 시나리오 (1번의 Update Step 내부)

> ⚠️ **FSDP (Fully Sharded Data Parallel)** 환경 기준 설명입니다.
> FSDP는 **모델 파라미터를 GPU 간에 분할**하고, **데이터는 각 GPU가 독립적으로 처리**합니다.

1.  **Mini-batch Sampling**: 512개 중 **16개**를 뽑습니다.
2.  **FSDP 분산 처리**:
    - 각 GPU가 **동일한 16개 mini-batch 전체**를 독립적으로 처리합니다.
    - 모델 파라미터만 GPU 간에 분할되어 있습니다 (메모리 절약).
3.  **Gradient Accumulation (GPU 내부 동작)**:
    - 16개를 한꺼번에 처리하지 않습니다. (`micro_batch=1` 설정 때문)
    - **Loop 1~16**: 각 데이터를 순차적으로 로드 -> 계산 -> Gradient 누적 -> 메모리 해제
4.  **All-Reduce**: 4대 GPU가 각자 모은 Gradient를 합칩니다. (동일 데이터이므로 평균화)
5.  **Optimizer Step**: 합쳐진 Gradient로 모델 파라미터를 **1회** 변경합니다.

```
FSDP 동작 방식:
┌─────────────────────────────────────────────────────────────┐
│  GPU 0         GPU 1         GPU 2         GPU 3           │
│  ┌─────┐       ┌─────┐       ┌─────┐       ┌─────┐         │
│  │모델 │       │모델 │       │모델 │       │모델 │  ← 파라미터 분할 │
│  │1/4  │       │2/4  │       │3/4  │       │4/4  │         │
│  └─────┘       └─────┘       └─────┘       └─────┘         │
│     ↓             ↓             ↓             ↓            │
│  16개 전체     16개 전체     16개 전체     16개 전체  ← 데이터는 동일 │
│     ↓             ↓             ↓             ↓            │
│  Gradient      Gradient      Gradient      Gradient        │
│     └─────────────┴─────────────┴─────────────┘            │
│                        ↓ All-Reduce                        │
│                   평균 Gradient                            │
└─────────────────────────────────────────────────────────────┘
```

---

## 5. 메모리와 OOM 분석

많은 분들이 **"n_agent=8 이면 8배의 메모리가 필요한가?"** 라고 걱정합니다.

### 결론: 아닙니다.

메모리 사용량을 결정하는 것은 오직 **`ppo_micro_batch_size_per_gpu`** 입니다.

- **n_agent=8**: "나중에 학습할 문제집 두께"가 두꺼워지는 것입니다. (RAM 사용량 증가)
- **micro_batch=1**: "한 번에 책상에 펴놓고 보는 문제지 수"는 1장입니다. (VRAM 사용량 결정)

따라서 `n_agent`를 16, 32로 늘려도 **학습 시간이 길어질 뿐, GPU 메모리가 터지지는 않습니다.** (단, Rollout 시점의 VLM 추론 메모리는 별개입니다.)

### OOM이 발생한다면?
만약 OOM이 발생한다면, 그것은 "동시에 여러 개를 처리해서"가 아니라 **"그 1개의 데이터(이미지+긴 텍스트) 자체가 너무 커서"** GPU에 안 올라가는 것입니다.
- 해결책: `max_prompt_length`를 줄이거나, 이미지 해상도를 낮춰야 합니다.

---

## 6. 학습 안정성 (32번 업데이트의 비밀)

"한 번 모은 데이터로 32번이나 업데이트하면, 나중에는 데이터가 너무 낡은(Outdated) 것이 되어 학습이 불안정하지 않을까요?"

### PPO의 안전장치
이 문제를 해결하기 위해 PPO/GSPO는 두 가지 방어막을 칩니다.

1.  **Clipping (클리핑)**:
    - 모델이 너무 많이 변해서 확률 비율(Ratio)이 `[1.0 - 0.0003, 1.0 + 0.0004]` 범위를 벗어나면, **업데이트를 무시**해버립니다.
    - 설정: `clip_ratio_low=3e-4`, `clip_ratio_high=4e-4` (비대칭 클리핑)
    - 즉, 32번째 스텝에서 모델이 너무 달라져서 엉뚱한 방향으로 가려 하면, 알고리즘이 "잠깐, 너무 갔어!" 하고 막습니다.

2.  **Small Learning Rate**:
    - `lr=1e-6`이라는 아주 작은 학습률을 사용합니다.
    - 32번을 업데이트해도 파라미터가 급격하게 변하지 않도록 거북이 걸음으로 학습합니다.

### 요약
- 32번 업데이트는 **"데이터 효율성(Sample Efficiency)"**을 높이기 위한 필수 전략입니다.
- 어렵게 생성한 512개의 데이터를 한 번만 쓰고 버리는 것은 너무 아깝습니다.
- 안전장치(Clipping) 덕분에 학습은 안정적으로 유지됩니다.

---
